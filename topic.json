{
    "start_timestamp": [
        "00:00:04",
        "00:00:59",
        "00:01:55",
        "00:02:57",
        "00:04:03",
        "00:04:45",
        "00:05:52",
        "00:06:56",
        "00:07:59",
        "00:08:52",
        "00:09:59",
        "00:10:56",
        "00:12:00",
        "00:12:59",
        "00:14:01",
        "00:14:59",
        "00:15:59",
        "00:16:59",
        "00:17:56",
        "00:18:45",
        "00:19:59",
        "00:20:55",
        "00:21:47",
        "00:22:55",
        "00:24:00",
        "00:24:49",
        "00:25:57",
        "00:27:00",
        "00:27:53",
        "00:28:57",
        "00:29:53",
        "00:30:59",
        "00:32:00",
        "00:32:59",
        "00:33:57",
        "00:35:01",
        "00:35:59",
        "00:36:55",
        "00:37:58",
        "00:38:46",
        "00:39:59",
        "00:41:00",
        "00:42:01",
        "00:43:03",
        "00:43:57",
        "00:45:04",
        "00:45:54",
        "00:46:59",
        "00:47:58",
        "00:48:59",
        "00:49:57",
        "00:51:00",
        "00:51:54",
        "00:52:57",
        "00:53:59",
        "00:54:56",
        "00:55:53",
        "00:57:00",
        "00:58:03",
        "00:58:47",
        "01:00:02",
        "01:01:00",
        "01:01:58",
        "01:02:47",
        "01:03:59",
        "01:04:59",
        "01:05:56",
        "01:07:01",
        "01:07:58",
        "01:08:55",
        "01:10:00",
        "01:11:01",
        "01:12:02",
        "01:12:55",
        "01:13:55",
        "01:14:53",
        "01:15:58",
        "01:17:00",
        "01:18:02",
        "01:19:02",
        "01:20:00",
        "01:21:01",
        "01:22:01",
        "01:23:01",
        "01:23:57",
        "01:24:56",
        "01:26:00",
        "01:26:58",
        "01:27:52",
        "01:28:56",
        "01:29:59",
        "01:30:58",
        "01:31:55",
        "01:32:57",
        "01:33:55",
        "01:34:57",
        "01:35:59",
        "01:36:58",
        "01:37:59",
        "01:38:55",
        "01:40:00",
        "01:40:58",
        "01:41:57",
        "01:42:59",
        "01:44:00",
        "01:45:01",
        "01:45:59",
        "01:46:55",
        "01:47:56",
        "01:48:53",
        "01:50:02",
        "01:50:57",
        "01:51:54",
        "01:52:54",
        "01:53:58",
        "01:55:00",
        "01:55:53",
        "01:57:01",
        "01:58:01",
        "01:58:58",
        "02:00:00",
        "02:00:54",
        "02:01:55",
        "02:03:01",
        "02:03:55",
        "02:05:01",
        "02:06:00",
        "02:07:00",
        "02:07:58",
        "02:08:59",
        "02:09:59",
        "02:10:52",
        "02:11:55",
        "02:12:53",
        "02:14:01",
        "02:14:59",
        "02:15:59",
        "02:17:00",
        "02:18:07",
        "02:19:14",
        "02:19:56",
        "02:20:58",
        "02:21:51",
        "02:23:00",
        "02:23:59",
        "02:24:59",
        "02:25:46",
        "02:26:57",
        "02:28:06",
        "02:29:02",
        "02:29:55",
        "02:31:00",
        "02:31:48",
        "02:32:54",
        "02:34:00",
        "02:34:52",
        "02:35:50",
        "02:36:55",
        "02:38:01",
        "02:39:02",
        "02:39:52",
        "02:41:01",
        "02:42:02",
        "02:42:59",
        "02:43:54",
        "02:44:55",
        "02:46:01",
        "02:47:00",
        "02:48:02",
        "02:48:58",
        "02:49:58",
        "02:50:58",
        "02:51:58",
        "02:52:58",
        "02:53:48",
        "02:55:03",
        "02:55:57",
        "02:57:07",
        "02:57:58",
        "02:58:58",
        "03:00:01",
        "03:01:07",
        "03:01:53",
        "03:02:50",
        "03:03:58",
        "03:04:59",
        "03:05:58",
        "03:06:59",
        "03:07:57",
        "03:08:58",
        "03:10:00",
        "03:11:01",
        "03:12:00",
        "03:12:57",
        "03:13:59",
        "03:14:47",
        "03:15:53",
        "03:17:01",
        "03:18:12",
        "03:18:53",
        "03:19:59",
        "03:21:00",
        "03:22:02",
        "03:22:59",
        "03:23:57",
        "03:25:02",
        "03:25:55",
        "03:27:00",
        "03:27:54",
        "03:28:54",
        "03:30:00",
        "03:30:49",
        "03:32:01",
        "03:32:59",
        "03:33:54",
        "03:35:03",
        "03:36:00",
        "03:37:01",
        "03:37:57",
        "03:38:59",
        "03:39:55",
        "03:41:07"
    ],
    "end_timestamp": [
        "00:00:55",
        "00:01:53",
        "00:02:55",
        "00:03:59",
        "00:04:44",
        "00:05:51",
        "00:06:55",
        "00:07:57",
        "00:08:50",
        "00:09:59",
        "00:10:56",
        "00:11:58",
        "00:12:57",
        "00:13:59",
        "00:14:57",
        "00:15:58",
        "00:16:59",
        "00:17:55",
        "00:18:44",
        "00:19:57",
        "00:20:54",
        "00:21:46",
        "00:22:53",
        "00:23:57",
        "00:24:42",
        "00:25:54",
        "00:26:59",
        "00:27:51",
        "00:28:55",
        "00:29:53",
        "00:30:58",
        "00:31:59",
        "00:32:57",
        "00:33:56",
        "00:34:58",
        "00:35:56",
        "00:36:54",
        "00:37:56",
        "00:38:45",
        "00:39:55",
        "00:40:57",
        "00:41:57",
        "00:42:59",
        "00:43:56",
        "00:44:58",
        "00:45:53",
        "00:46:58",
        "00:47:57",
        "00:48:58",
        "00:49:54",
        "00:50:59",
        "00:51:49",
        "00:52:55",
        "00:53:57",
        "00:54:54",
        "00:55:52",
        "00:56:58",
        "00:57:58",
        "00:58:44",
        "00:59:59",
        "01:00:56",
        "01:01:56",
        "01:02:45",
        "01:03:57",
        "01:04:56",
        "01:05:54",
        "01:06:58",
        "01:07:54",
        "01:08:52",
        "01:09:58",
        "01:10:59",
        "01:11:59",
        "01:12:54",
        "01:13:54",
        "01:14:51",
        "01:15:56",
        "01:16:59",
        "01:17:53",
        "01:18:59",
        "01:19:57",
        "01:20:59",
        "01:21:58",
        "01:22:58",
        "01:23:56",
        "01:24:55",
        "01:25:54",
        "01:26:54",
        "01:27:50",
        "01:28:53",
        "01:29:56",
        "01:30:54",
        "01:31:54",
        "01:32:57",
        "01:33:54",
        "01:34:56",
        "01:35:58",
        "01:36:56",
        "01:37:57",
        "01:38:55",
        "01:39:57",
        "01:40:56",
        "01:41:56",
        "01:42:58",
        "01:43:59",
        "01:44:59",
        "01:45:58",
        "01:46:54",
        "01:47:54",
        "01:48:50",
        "01:49:59",
        "01:50:55",
        "01:51:53",
        "01:52:54",
        "01:53:57",
        "01:54:59",
        "01:55:52",
        "01:56:58",
        "01:57:59",
        "01:58:57",
        "01:59:58",
        "02:00:53",
        "02:01:53",
        "02:02:59",
        "02:03:54",
        "02:04:59",
        "02:05:58",
        "02:06:56",
        "02:07:56",
        "02:08:59",
        "02:09:56",
        "02:10:50",
        "02:11:53",
        "02:12:45",
        "02:13:56",
        "02:14:55",
        "02:15:55",
        "02:16:51",
        "02:17:37",
        "02:18:48",
        "02:19:25",
        "02:20:57",
        "02:21:50",
        "02:22:59",
        "02:23:53",
        "02:24:56",
        "02:25:44",
        "02:26:55",
        "02:27:55",
        "02:28:58",
        "02:29:54",
        "02:30:58",
        "02:31:45",
        "02:32:52",
        "02:33:58",
        "02:34:51",
        "02:35:48",
        "02:36:50",
        "02:37:59",
        "02:38:59",
        "02:39:47",
        "02:40:58",
        "02:41:58",
        "02:42:57",
        "02:43:52",
        "02:44:48",
        "02:45:58",
        "02:46:58",
        "02:47:58",
        "02:48:55",
        "02:49:56",
        "02:50:56",
        "02:51:56",
        "02:52:57",
        "02:53:44",
        "02:54:59",
        "02:55:55",
        "02:56:50",
        "02:57:56",
        "02:58:55",
        "02:59:58",
        "03:00:55",
        "03:01:52",
        "03:02:49",
        "03:03:57",
        "03:04:58",
        "03:05:55",
        "03:06:58",
        "03:07:57",
        "03:08:58",
        "03:09:59",
        "03:10:57",
        "03:11:58",
        "03:12:56",
        "03:13:57",
        "03:14:45",
        "03:15:51",
        "03:16:58",
        "03:17:58",
        "03:18:51",
        "03:19:59",
        "03:20:59",
        "03:21:59",
        "03:22:57",
        "03:23:56",
        "03:24:58",
        "03:25:54",
        "03:26:59",
        "03:27:53",
        "03:28:53",
        "03:29:54",
        "03:30:48",
        "03:31:57",
        "03:32:56",
        "03:33:51",
        "03:34:54",
        "03:35:59",
        "03:36:54",
        "03:37:55",
        "03:38:57",
        "03:39:55",
        "03:40:59",
        "03:41:51"
    ],
    "context": [
        " So it is a try and look at what all things we have been done.  So as part of our journey we learned that we have to follow Chris DM or the latest one is Chris and mal que methodology, let me write crisper and Q amp D.  And as part of this process management methodology six phases or six steps are there.  And the first one is business and data understand that.  This is more about gathering requirements and doing the planning.  So here we have.  planning.  Gathering planning and researching this.  posters, we learned about data pre processing.",
        "  This is where we try to clean the data modify the data, if required, so we are we're basically doing a lot of manipulation syrup with respect to the data that we get and had.  This is the most time consuming step.  That.  time consuming step.  After data processing.  We come to data mining steps.  And this is where we are actually currently working with operating.  Under data mining, we learned that there are unsupervised learning.  Here we are not trying to do predict right.  There is nothing like predicting any specific variable, so we will say bye is not known.  This is not.",
        "  So if I don't have labeled the data, then, are not be trying to work for that labeled data right, it is all about identifying the hidden pattern, the date.  And then we can do supervised.  But before I talk about supervised learning, we also analyze that whenever we get the data we start with you need a period analysis.  In order to understand the data and understand the details matter we try to look at uni variance analysis one variable at a time, and we do add on top of that, and by visualization.  graphical representations.  Post this we go to by various visualization.  This is where we talked about two variables, taken together, will try to visualize it using scattered back.  or try to identify the Cross table.  Right confusion matrix kind of a thing we will get a pivot table.",
        "  Then we have multivariate analysis.  So while while we were doing these things.  That we learned about various techniques, for example, all the unsupervised learning techniques are basically the techniques for doing the analysis in multi variable case.  So clustering dimension deductions and all those concepts that we discussed association rules for all of them are for falling under you need a nexus.  Post this.  Right post, the analysis, we will also want to identify or predict the results and that's where we basically talked about supervised learning, and here we are saying label to data is there, so that is why he is known.  Because I know what is the result, try to establish the relationship between the existing factors and.  Dubai.",
        "  Right now under this scenario.  If I have to predict why, which is numeric.  Then we apply something called as regression techniques, and this is what we have to start learning.  If I is non human.  If I is non numeric then we look into classification.  Classification techniques to a numeric non numeric regression or classification.  Under classification techniques, we learned about.",
        "  knife based technique which runs on probability based concepts conditional probability, then we looked at K N, which stands on the distance logic nearest neighbor and then we discussed about decision to this.  This is a rules based concepts.  We also talked about.  The complex models of n simple techniques.  That we looked at and several techniques, this is nothing but a combination of multiple models and in and sample techniques.  In and simple techniques, we looked at.  four different variations.  Voting.  stacking.  three models bagging.  And finally, we had boosted.  Voting and starting can be non three more based models as well.  Sir, sir.",
        "  yeah so even like I was like looking at the fedex data set so like you know we were asked to give you know evaluate or use the data set on different models and like.  Tell like whichever model is having the highest accuracy like we can deploy that model so yeah like we can like use separately can and knife base and all or we can do, stalking as well, no so like if you're just stalking obviously the accuracy will be much more than the individual.  No, no, no, no, no, no, no.  How can you say that stacking will improve the molecules.  Logically, should be but let's say if I have model, one model to model three. three models are there.  Which are individually predicting on your fedex.  OK OK.  So this will give me reserved one, this will give me reserve to this is giving me reserve.  Now, what we are trying to find out is the value of our variable of our interest correct.  Yes, so as you.",
        "  This model, one is giving very good result that is a very good, it is very close to the actual data.  Okay, but these two are not so good they're deviating from the original.  Okay, now, when you apply Meta modern final model.  they're basically taking these as inputs and trying to predict why.  The final result.  Okay.  But because these two results are two and three are very, very deviating from the original data when you do average this good result will also be pulled away.  Yes, yes okay so.  This challenge will always be there when you're doing.  your thoughts are on that that is the reason why we evaluate individual models also.  Okay.  And then we compare it with a final assemble model.",
        "  So what if we need to do the individual products check individually and again to stalking also and then, once again verified. Yes.  i've seen.  And like so it's like the problem statement is like are not correctly precisely what do you want is not like correctly given so it's like can we do like our own stuff like we can also have network analytics right because it's like source.  In this.  nation is there, so we can do that. We can.  yeah he's a logic.  The logic of fedex data set right given as a project is for you people to do many things.  So many things you have to also do business understanding.  Yes.  misunderstanding So yes, so i've created a separate document so it's like i've written the what they call the objective like minimize maximize constraint, then prepare the data dictionary.",
        "  Then, after that it's like like I thought i'd write, like every pre processing steps like why i'm doing it in the document so that it is clear, like every like data mining and also have just prepare that document.  says Oh, I thought i'll go through it.  yeah see.  There is no standard logic, I mean I need, but as a data scientist.  You have free will basically in any organization as a data scientist, if you are hired right, you have a free will to look into all the aspects of the business and identify into the areas where there is a problem.  Okay, so it's like yes so.  look good good.  is essentially a good objective so, is it sufficient if we can just try to minimize and maximise like what do you want to minimize and what do you want to maximize because you said, like octave optimize terminology should be there should be like very short.  Okay, it is, it is see it is not mandatory, that you have to have them, it is recommended that you need to.  Okay, but it's like we don't have to give elaborate view of the objective right it's just that was.",
        "  Not an objective is short crisp and optimization terminologies are recommended.  Okay right, you might also want to explain what exactly you're trying to achieve.  Okay, does this right a statement for.  evaluator as a viewer if I see that statement.  I may not be able to understand what what you're trying to do.  Because I have not researched on the data right.  Yes, you.  have to give an explanation also so.  That it is clear for the person who's reading.  Okay, so what exactly i'm doing on the data like beat anything that work at analytics or you know distance and delay relation whatever we're planning to do, I can just put it in that document.  So just explain.  Yes, yes, yes intimately you have to.  give some insights to the business or the management.  Okay, they can do better was.  Okay last thing can be done dimension reduction can be done, like you said network analytics can be done.",
        "  Yes, right and then classification models are there.  Yes, okay.  Okay okay so okay so.  Okay, so we're talking about this classification models and here we have voting stacking bagging and staying.  right here under bagging again we discussed about.  Something.  Which is very, very.  efficient way to build things which is random forest.  The difference here is, we will take a sample data which is.  smiling bike capital m, this is your sample data for batty but for random forest, we will take subset of features as.  Good that's the difference.  Okay bad, there are no for us, this one variant of.  Then we'll talk about boosting.",
        "  Under boosting we have.  Added boosting.  Then gradient descent and gradient boosting and then we have extreme gradient boosting and these models.  That try to minimize the error.  By taking the models sequences by training the model sequence.  Here we take multiple sub samples and do training parallel.  that's the difference between boosting and bagging techniques.  So this is what we discussed Taylor.  Now, if I talk about.  Right being numeric piece.  Okay, why is numeric.  If this is the case.  Then.",
        "  We will try to go with something called as regression.  But before I go ahead and deal with regression models, you need to try and understand certain things that that we discussed in the initials.  So when I say why numeric.  Why can be continuous or discrete okay numeric data can be continuous our district, we will now stick to the continuous case okay.  If I is continuous.  Then there are certain properties you remember this properties.  Yes, so I think it is mobility should be equal to zero always range will be like between minus infinity plus infinity.  And I think he area undercover will be equal to one and.",
        " Something.  yeah this is.  This mean mean median and mode or equal.  know that is when you have submitted the curves and.  All that that is normal distribution okay.  we're only talking about continuous case.  Okay okay So yes.  Okay, of course, normal distribution and all that are our.  Next steps.  Okay, so if I have output variable y which is continuous in nature.  Then, these are the important properties that we have to remember minus infinity plus infinity probability of identifying any single number is always being equal, to zero and area under the curve is always equal to one.  click now.  let's say, am I asking you to predict something when something.",
        "  Okay, for example, months.  In the classification scenario we identified whether the student will pass or.  fail.  Okay, good I didn't define whether the student will be passing or failing, so this is non numeric it.  will apply classification model.  But here we are talking about identifying the exact marks which is numerical.  Now, if you apply a model and try to predict that so you're predicting okay whenever.  you're saying for a student the marks will be let's say 85% that the student is getting 85% marks.  If this is the case.  Right.  I mean if you're predicting.",
        "  What is the chance that the student is actually getting a defect marks. yeah.  What is the chance that the student is actually getting 85%.  anyway.  let's say i'm predicting the marks for you.  i'm saying that you will get a difference, what is the chance that you will actually getting a difference.  If you don't know the student and probably percentage is anything out of hundred right so 285 is only a one number.  If you are taking in terms of decimal points like 85 point, probably to be one by infinity which is like we cannot predict zero but.  If you are considering it as though yes.",
        "  We are talking about marks which is.  Continuous, then it is zero so.  Exactly, then it is you know, the chances zero, then why the hell i'm doing prediction if I know that the student is not going to get 85%.  That means whatever i'm predicting is wrong isn't it.  These are the properties right for continuous data, these are the properties and and trying to predict the value, which is continue as and we are saying that the chance of the value zero.  Then the prediction.  If you have historical data in the historical data like we can see that the student has like previously got like a lot of 85% ages, then we can try it in that way.  But we are dealing with.  population data here we're not dealing with.  subset of data if it is a subset of data, then it becomes discrete, it is not continuous anymore, yes.",
        "  There are fixed values right So if you have hundred samples and hundred records that means you know exactly what is the value, so it becomes discrete if discrete there is no problem, we can do the right prediction classification models have been taught.  That understanding, where i'm going here.  Yes, yes.  So for a continuous data.  How can I predict a single number.  Right, because we are seeing the probability of a single number exit identify a single number on this infinity scale is zero.  So how do I deal with it the continuous Dudes that that is all about this logic that were trying to understand, yet scholars confidence interval.",
        "  So, to start with continuous data, these are the three important properties that we discussed right area is equal to one property of a single number is equal to zero, and the range lies from minus infinity plus infinity if I extend this that Okay, these are.  The data is actually normal.  I mean.  they're not assuming that there is any abnormalities then it also has these two properties mean median equal to more.  And then area is symmetric the curve asymmetric right and we also discuss normal distribution properties.  Where the characterization of the bell shaped is done using these two.  Right values, new and Sigma first moment and second.  And we've also discussed about standardization.  X minus meal by Sigma This is basically trying to transform the X values to the values.  That we have we're moving the experiments to see by.  Training.",
        "  So these are all things that we have discussed, and of course we've also said that the.  compassion right any data which is.  Continuous has to be assumed that it is normal.  But is it actually normal is verified using normal qq plot.  Right and the Z values and the sample data should have identical distribution similar distribution.  If you do that.  You get a straight line and straight line basically means.  That the days normally is to be dead.  These are all things we discussed right.  Okay, remember.  Okay know.",
        "  Why, we are learning all these things, let us try and do a small example of doing prediction first how this prediction book right, so let us do the prediction.  Then we will see the application of that right prediction calculations into another exam.  So here, I want to take a small data set.  Okay, this is the data set so we are looking at the gmat score.  Okay.  So one note.  Here good good good.  Actually, like therapy on the continuous data you're predicting students marks that the probability of your student getting 85% would be zero like if we have continuous data.",
        "  And if we have, like previous historical data like not particularly about the student but till now, some population data like based on the attributes like new record similarity with the old records, we can get the actual day like we can predict the percentage rate.  So that is similarity read it is not prediction.  Okay, like normal machine learning algorithms for a supervised learning like that is what we have done by numeric.  numeric data, we are not seeing.  The regression regression models.  that's what we are going, we have not done that regression model yet. Oh.  This world on let's let's discuss this here you got to know oh.  yeah.  So let me load the data set. here.  i'm talking about this specific gmat score state Okay, let me do histogram.  yeah G man, this is the is to go.  Which is histogram.",
        "  And the curve is basically left skewed.  Okay.  Now.  Given this case.  Right.  Remember, whatever data, we are working is always.  Given this sample data.  Can I make some strategies and applied for the university for admissions.  And we've already discussed that because of the ship, we can conclude that the students right is left skewed us.  Because of the shape or left skewed behavior, we can clearly say that the examination was fairly easy to see, and that is the reason why we have not have basically people scoring high marks. Okay.  Now.  Given this case let's say.  You want to come up with strategies for admission.",
        "  Get students admission this year.  Right.  So each university will have their own strategies for admission right.  Given this data, what What would you basically come up with what is the plan that you will come up for.  Getting the.  I should say.  The admission strategy right, what does the education strategy that.  yeah guess what what strategies, would you want to.",
        "  We can say is the exams over then make it, we can set the cutoff like depending on the market, which we can see my histogram like the peak like where it is a number of candidates, we we want to take for the university, we can set a cut off.  Very good, very good lifted another seats weekend and you have to basically free them up, and there are 773 students who have.  students who have.  Written the exam sample data, this is sample data we have.  Okay, and we also looked into.  What is the minimum, what is the maximum maximum is seven at memon minimal mistakes and you can see that in the X axis good.  Okay, so admission.  Then you have certain serendipity students minimum maximum we have mean, and we also have standard deviation as well.",
        "  It means is 711 standard deviation is 20.  So, because I know the data we are coming up with all these PICs.  Right now.  If I have to let's say this is one of the universities that you are working as a team, you have to come up with the strategies, then that university will have some standard.  So you not ella every student who has written the exam Center is a qualification well, so all those people qualified for higher education so are there.  Right.  As a university right you don't want everyone to enter into your you could have cut off.  Another question is what will be the cutoff.  get what will be the backup.  How much.",
        " You put us kind of.  Or maybe depending on the top Marco thing by students, like some.  Top 10% or 90% of the students will be alone and.  have to be very specific top 10 or 20. yeah.  it's not there, then we can like see like based on the ranking like the top hundred.  Exactly Okay, this is only sample data.  Okay.  You have to generalize district.  Okay top 10 top 20 that is good.  That strategy is good.  let's do that.",
        "  So, in order to decide see again, we are trying to be specific, with respect to the idea, not with respect to the values right optimization techniques basically talk about the idea being optimized and the best result can be captured.  result will not be fixed it will change, based on the factors that we are considered okay.  So if I say cut off.  Okay.  Then.  What should be the ideal cutoff for this year.  Right, because the data is from the CSM and the the sample data that we're looking at is for the CSM.  So if I see.  625 is the cutoff then is this going to be a good quarter.  Because cut off means we are trying to filter out those poorly performed students.",
        "  right and as a university, you might have 625 or you might also have six at as your standard.  So seats at.  This no.  No it's not.  A very good cut off no Sir, because there are a lot of students who are have done very well.  And like if the universities are top grade university then.  The cutoff should be like probability probably higher.  let's say this is the university which isn't top 10 top 10 books okay.  i'm impressed us.  Okay, should we.  should be higher, because many students have done and.  The question is, if you're putting six APS, the mind.  Then, what is the percentage of the students who are getting eliminated marks.  Fine you're seeing from 600.",
        "  To seven ad, this is the range and you're seeing success, it should be the cut of that means anyone who scored greater than six he will be qualified anything, who has less is disqualified very good, but how many people you are going to filter out.  hey What if your graph is like this if you say six at.  don't you eliminate a lot of people.  The 6600, and this is 785.  Difficult examination, so in that key is a lot of students who scored around around about 680 or less so the percentage of the students who get eliminated is going to be very high here.  Right and.  If this is the case.  If this is the case and if I say six at here very, very limited very small number of students are getting filtered.  It means a lot of people are still available.  isn't it.",
        "  So you have only limited seats hundred hundred seats are available, so if I to six ad Am I filtering out a good amount of students or not.  Right, I said top.  10 right Mona Lisa i'm assuming a lot of students are applying.  Right top 10 universities, a lot of people will have.  So the decision from the management has to be checked again every year, based on the distribution of the data.  Okay, if the examination is easy at this is the kind of distribution, you will see in this case the cutoff could be somewhere here if the examination was very difficult, then obviously a lot of students will score less mark so you're cut off should probably be around this way this space.  So that you give majority of the people right the apple logic.",
        "  That the logic is you, you allow a lot of students, good quality students to apply for the university.  Now, can you tell me if I choose six at.  How much percentage of students are getting eliminate debt.  So if I have to get back remember area under the curve is one so we're basically saying.  This is the bell curve.  And i'm interested in this area.  Okay i'm interested in this potion.  know how do I calculate area under this curve.",
        "  Because remember how do we calculate area.  Yes, how many of you actually remember calculus integrations.  So this is.  This is upper limit 680.  And then I say fix.  This is a representation right.  But the point is, what is the equation, we have the equation given.  Well, so actually that's what I was thinking.  Exactly and.",
        "  Another challenges for every time the data chain.  Is, we have to get the equation again done right this entire area has to be calculated.  So for every small or big change, you will have to do a lot of mathematical calculations and there are so many different right continuous variables and for all of them, you don't have this equation equation was given fine can do that.  So what is the alternative yet what can I do.  So here in order to capture the area under the curve, we will now comes the normal distribution guys, this is the application of normal distribution Center.  We are verifying i'm raising all this. stock, let me just erase all this.  unwarranted thanks.  Alright, so we're talking about.  Normal distribution.",
        "  Why, because this is continuously.  Of course, you know, to verify using normal qq plot or not yeah sooner than that and data is normally distributed to your G man data is normally distributed.  If this is normally distributed.  Right.  Then I can try to map it with Z scale.  isn't that means i'm trying to do standardization.  I can do that right.  Normal distribution G man, and then we are talking about standardization.  I can do the standardization, because you might as normal distribution, remember that.",
        "  So for every corresponding gmat score.  For every corresponding gmat score, we will have a corresponding value nz.  But this is new and this is new, which is equal to zero, this is 711.  And the standard deviation yet on see we have Sigma as one.  Whereas on G matt the Sigma was 20.  So i'm trying to map like.  We are trying to find out the area under the Belka for six a D.  So what is the corresponding Z value for 686 80 minus newest 711 Sigma is 29 these are standard values we already know data, so we can.",
        "  Get yeah can anyone tell me what is this reserve six at minus seven seven by 29 i'm taking the time to.  pick C D minus seven.  minus 1.06.  minus 1.06.  degrees. minus 1.6.  hey so for X value that is Jim add value to succeed if I have to compare that with Z then on Z we have minus 1.6 right, but what is the.",
        "  Z is this standard.  which follows normal distribution of course.  Because it is standard.  It has certain property static properties right standard properties.  If it is standard.  Then statisticians right can also do a lot of calculations.  Because it has certain properties doesn't change so.  Using this logic statisticians have calculated.  area under the curve for different different Z values, because why Jesus symmetrical curve at the Center you'll have zero.",
        "  So I will be able to calculate the area under the curve this entire area under the curve is one but for a specific value what is it specific value what is it right, so these kind of calculations are already done, and they are stored in something called lessons Z table.  So let us look into the Z table here.  Let me minimize this.  There is excluded it empties Z table remember this is a standard so fixed.  This thing will be.  The table.  We are trying to verify.  But this the table as we discussed is actually carry.  This Bush.  left side portion.  From see.  I could do the calculations.",
        "  Because it is standard right.  So using this concept we are trying to now do the calculation for. 60.  Zero minus 1.06 you guys did that calculation, this is the Z table.  happening, the link in the.  chat zoom chat you can just open it it doesn't the website it just search for 316 digit n g.com slash CT but you'll get this.  So when when we look at this.  I will have.  Z values on first corner.  And you can see, the rules each show is speaking about one different Z value.  And the second decimal value this is with respect to first assessment, but the second decimal values are expressed in barrels.",
        "  Okay, so we need minus.  1.06 so how minus one yet.  minus 1.0 and 06.  that's that's what we're trying to get.  So minus one is here.  And the second step disabled 06 right bring them together.  I get 0.1446 if I multiply this with hundred i'll get 14.46%.  Now, from a university perspective if you are applying cutoff.  Are you willing to lose. say.  Are you willing to lose 15% of the students.",
        "  That that's The point of this.  they're trying to see if you can lose 15% of the students.  If you if your management says no 15% is very big number I don't want to do so many people.  Then, probably, you might want to reduce the society to a smaller value 60 or 70 years 75 whatever that is.  Right.  So if I say 660.  yeah can you guys tell me.  What will be the percentage of students who get filtered out if you choose the math to be less than 665 let's say 660 yeah because the minimum oxen 600, so I would try to see if it is 665.",
        "  yep, then what is the percentage of students, that we are using yeah.  Do the calculation of.  10 minus 1.58 so we have to check for like minus 1.5.  And the minus 1.5 and it so.  Yes.  So it's these 7.1% is it.  Is a.  high point sorry 5.5 point 7%.",
        "  Okay, to side with did everyone get minus 1.58 as a calculation for Z.  So we were basically having X from this X we're trying to calculate Z.  How by using the formula X minus MU by Sigma I already mentioned newest to 11 Sigma is 29.  yeah did everyone get the same result minus 1.58.  If yes, then good.  After this, we are looking at.  The table.  And we try to find out the value, which is right falling on that specific ad.  05717 to 11 you have written.  It was 711.  yeah okay so remember seven.",
        "  yeah if this is the case.  Then the area for this particular program is.  5.71 percentage.  Okay, so you're you're basically looking at a scenario where using the Z table standardization formula we are trying to assess the area under the curve and what we're saying is gmat score is normally distributed That means it will be similar to yours see.  That the assumption right data is distributed normally means it is just like shape of the curve is just like see it may not be exactly the same, but we're saying it as similar.",
        "  Given this case, if I have to find out this area, then I tried to map that same value on Z and then i'll calculate this and that is nothing but you are.  clear to everyone.  Any questions.  Now.  yeah no go ahead.  Of.  The curve is the constant function, no, no, no.  No.  Come again.  Hello.  Hello.  I can hear you. repeat.  Yes.",
        "  Sir, I just said, you know you got a question of this curve is the cost function like.  So one thing I would like you said before that the one thing I would like to know the significance of negative geez energy value.  Or the positive value So can I interpret the the general things because somewhere in the G value is summer i'll get the positivity value and summer i'll get the negatives so.  there's nothing specific to interpret this is zero.  ne ne zero.  The values are spread across the sea table.  Because i've actually or how can I say that, as you said, there is a lefty skew the data that the concentration of data is you know concentrated towards the right side it's left to school, yes, because the.  mean as influential.  Know loads loads and loads of things.",
        "  Yes, but, but I see that.  The Z de mar data is not normal did I say that, or what was the discussion that we were having.  dinner, assuming that decision.  Okay.  So, if it is normal, we don't consider this kunis and all that we are assuming that everything is like a bell shaped curve.  Okay, practically.  Practically.  Even though G matt is showing a little kunis like this.  But when we try to look at or when you try to compare jima data with.  Standard Z value.  Then we find that the line is straight on the normal qq plot.  Normal keep.  The data scattered in a straight line that means we are assuming it may not be perfectly straight, but overall pattern is a street and.",
        "  That means the data is normal.  This thing can be applied the logic that we are doing here right.  we've been very basically we're taking X, then we are trying to calculate the Z, then we are trying to calculate probability using Z table.  All this will be valued only when this is normally distributed, only then I can compare it with see right.  or.  If it is not normal and there is no point I do the comparison, because obviously this is not normally to skew.  Whereas Z will have a perfect will ship.  Right.  Thank Thank you.  So we are assuming that data is not but.  Only then we are doing all this.  Now.",
        "  I want also to give scholarships.  right as a being if I have to strategize something for admissions I also want to give some offers.  But these offers i'll give to only good performing students because eventually they are going to get into good jobs and they're going to get name to the institution to the university.  So we are we're trying to run offered scholarships, based on the performance.  Okay.  Another question.  Can we consider like person tied in this case of scholarship like 90 percentile students who scored on 90 percentile or something like that.  Yes, yes, yes, that is what we are doing.  OK OK.  So Vanessa six at.  It is actually a point that point is percent.  below that you have 15% page.",
        "  OK OK OK, so at.  This point becomes 15%.  Okay, so.  same day, I also want to give scholarship.  only to those students who have secured very good.  Right.  So how about.  coming to a conclusion on what value should.  Here we use succeed on the left side.  But on the right side.  Maybe i'll say.  The maximum Marcus 780.  So i'll give.  scholarship to let's save.  Seven 770.",
        "  i'm going to tell that all the students will get scholarship if their score greater than 70.  Now, if I declare this, then how many students are going to fall in that section, because all for all of them, I have to give the scholars, will this be a good idea to randomly just declare the scholarship based on them.  Again, because if it isn't easy exam.  lot of students will fall in this area.  Linden.  If this is the GIs then obviously i'm going to go into losses because free educational scholarship right.  If you're declining scholarship then a lot of people are getting into.  This bracket.  isn't it so in this kind of scenario, maybe I want to reduce this reach to a smaller value.",
        "  Right So how do we deal with this, can you tell me.  Can you tell me.  What is the.  percentage of students who have actually got marks greater than 70.  yeah.  So for that we need to know, like the data right like the number of students who got what do they call.  About 770 and on by the total number of students.  Did you get the.",
        "  same number of students who got say the much less than 630.  OK OK OK so i'm going.  To do the calculation.  Yes, yes, Sir, yes.  So, so the Z value is 2.03.  So.  We need to check for like the percentage of 2.03 for the Z value.  yeah you're absolutely right we'll open it open it.  Okay okay. yeah whatever and others.",
        "  Okay, good Okay, the bus responded.  said I got my answer is. 97.8890407 72.38%.  Less than 2% will be people that she.  Very good, yes.  see what we're trying to understand here is greater than service.  So if I have been shaped curve and saying this is 770.",
        "  So when you use the seat table always remembers the table does the calculations, so this is lower limit then upper limit and then function of X dx so always it takes the.  left side Bush and oversight for doing the calculations very so try say seven seven D, it is assuming that from here, everything.  read all this data, and when you refer to the table you're getting somewhere around 98% down to the ground, it off.  But i'm not interested in the Left set portion interested in greater than.  data and so that means we're interested in this portion.  The end of the total curve is 100% right.  So we are taking only 2% by removing that 98% of the data.  Good.",
        "  Is it clear. Yes.  So the table is actually referring to the left side portion of Z.  cumulative portion.  So the new look at that we are getting 98.2% 98% if I convert that it is to push.  It off right and shaded push right, I should say area of the curve, where you have red colored lights.  So that means, if I can.  Have a budget.  Then I can decide what will be the cutoff values.  Right in order to get the area and the local and area under the curve, is nothing but your probability or portion.",
        "  So this is how Z table is used for to indicate.  Okay, but always remember it is only looking at left side portion of this. Okay.  Okay now.  Using the standard ization for.",
        "  Using this standardization for law to map the X values to see when US and because these standard the calculations are already done for this table using this as a reference, we are trying to evaluate the values for the kind of it.  Okay, these are the conditions that we talked minus 1.06 it is 14% if I have to want.  It and these calculations, if I have higher value than we say greater than value and that's I think what one minus the left side portion gives you the right side push.  But all this is possible only when your data is normally distributed if data is not normal all this doesn't make any sense, so if data is not normal What should we do, or what we do.  yeah if data is not normal.  What we do.  We discuss this long time.",
        "  If data is polled that form to be non normal.  Always normalize the data right.  But what is the process called last.  normalization or.  Standardization standardization is continuous.  it's not normalization of standardization.  As one solution, I think.  i'm summations very.  Good, the answer is transformations.  We transform.  The Non non data to novelty once we get the transform data we do the calculations get the results transform it back to those you know scale and then evaluate or interpret the results.  yeah right, so this is normal qq plot strategy without normality, we cannot proceed for.",
        "  Okay, so next is very important point we'll discuss that in a moment we'll take shortcuts yeah this time i'll give you more brains, but shorter duration.  So in my watch it's 373 to five minutes break come back with me okay.  yeah.  Everyone.  Alright, so we just discussed on how we do calculation.  To get area under the curve.  This is nothing but the probability.",
        "  Now this probability calculations are useful for doing for addiction.  As you can see.  If you have do you mad scores.  Then, what is the chance that the student that I had randomly pick is having.  marks greater than let's say seven but less than 700 right, so this way we can try to use this area logic to calculate probability or prediction.  And this this concept is basically applied for value, which is continuous in nature.  And in order to do this calculation, we look at.",
        "  It we are looking at the normal distribution properties why because the value is continues, and if it has continued as it has to be normally distributed, if not we convert that to become not but and once we have normally distributed data in we compare it with Z.  And nz we basically use the table.  we're using the table in order to calculate the area under the curve and if I know the potion.  With respect to the corresponding ziva you have X, then the area under the curve will remain say.  irrevocable remains.  So this this Z table is Alice to actually calculate the probability or area for your random variable X.  So that's normal qq plot for us.  Now.  This is with respect to you need a good analysis Okay, when you have a single variable we tend to do this.",
        "  But let's say you have multiple variables and all those those cases are coming.  Okay, and what we're basically saying is when we are trying to do the prediction.  Trying to do the prediction will always work with sample data, this is your inferential statistics concept will never get population data, the 773 students that we work with.  Right.  This is also a sample, this is not population. Okay.  Now let us assume.  I give you.  Data let's say this is my data okay 123456.  A day.  Okay.",
        "  When we talk about die, it has six possible outcomes, this has made our number 36 possible.  Okay.  If you basically let's say.  Roll the dice.  Four times, or three times.  Roll that particular day three types.  Right let's see which now has done the ruling of this day three days.  Okay, so this is my sample one randomly you might get one result for each direct and bedroom so give me some three random numbers out of the six assume that you're rolling the dice you got three results right so from that perspective.  Three five and 635613 times.",
        "  So you're basically having the results as three, five and one now let's say no, you don't. notice.  repeating this exercise, you are you're also randomly repeating right, so what might you get.  Give me three numbers.  And it's all good.  yeah.  Next manali couple you might want to get your own.  Yemen your your experiment results.  yeah see see for you guys.  Okay, want to give the same results okay.  it's for.  yeah probably you might want to give them.  A six four and three okay.  643 random guys don't give it too much of thought. it's just random output.",
        "  yeah so sampling five samples except they cannot so everyone asked me one so just bear with me on that so let's see we have 100 people and i've asked all the hundred people to do this experiment.  For all the hundred people, we will get random visits right, it may baby, or it may be same also lights you don't know.  Each of this role, I experiment is a sample.  Right.  Now, on this sample.  We will be doing the calibration.  Okay, now, what is the average of this.  Have it will be three right.",
        "  What is the average of this.  11 by three probably it is 3.6.  This one again three this one.  That team point 3014 four point.  So on so forth i'm getting different results.  What is the average of this actual population data.  yeah what is the other job.  All right, to bring something.  Different something like 3.5 3.5 average of 123456 is middle value 2.5.",
        "  So what i'm trying to explain here is when we do inferential statistics when we're trying to do inferential statistics.  We always work with sample data only but that sample data, maybe.  Your sample data itself might vary.  If you're on the line data various the statistics that you are going to get will also very.  Good.  isn't it.  Then among all these different samples that I have has 100 samples.  Out of all the samples who sample should I choose because I don't know this and i'm trying to calculate it from a business perspective.",
        "  Right so, for example.  I want to identify a standard example I want to identify it employee's salary I love this exam.  All the IP employees salary is what I want to identify let's say in India will will limit limit to India.  This is your population return you're interested in finding population parameters.  But, will you get all the employees D is will that be possible.  If I asked me people to do this.  What will you do.  You probably probably try to write conduct this way.  and gather data.  Because you're not have got a right readily available date.",
        "  And nobody directly discloses the salary.  So trying to ask some people, and some people may respond some people may not respond.  Okay.  So gather data.  Now, do you think.  The surveys right the data that let's say de Monte is going to get.  will be the same data that everybody will also get God Murray manali in the report now everybody.  i'd be getting the same data.  Know Dave is doing his own survey he's trying to reach out to his network people right.  Maybe his close associates close friends immediate family members so on so forth we're working in it department, there was going to get information from those people.",
        "  hey Similarly, if I talk about let's say permission permissions going to get the same information from this net rockstar is going to get the data from right that that particular network that that she is associated.  Right.  But.  Everybody is going to talk about.  Population parameter.  Linda they're trying to represent inferential statistics means that only right take statistics and then estimated to population.  Now among all your calculations, whose calculation, should I choose.  is no current Dave carrot carrot carrot who's correct.  Everybody is correct, because the logic that knowing is correct.",
        "  But because the underlying logic underlying data is changing.  The results are also going to change, so you cannot you cannot basically consider different different results for population parameter the parameters here is always fixed I mean one result or a parameter should be one reason.  But that's not gonna happen because samples are changing.  So, if this is the problem when we are trying to deal with inferential statistics and this problem is called as sampling variations or sample variation.  If this is your population.  As you perform and is assume this is your population.  Whenever we say population we're assuming that we don't know that.",
        "  But on this population, I have to calculate view and then for these numbers, the population is 33 because I know the data, you can also do the calculation, the right values will be at the to.  know in order to calculate the population.  If I don't know the population, then we will go first sample data assuming sample sizes to collect the data and then these are all the possible samples that I have.  Not depleting if it has to be known to pretend that these are the samples that I have on each of the samples has equal probability.  you're losing unbiased techniques right.  So each of the sample will have equal opportunity to get select.  Okay.",
        "  You may end up getting sample one or you may end up getting sample for our sample file or any time.  So if I have all these multiple samples, we may end up going into any of these samples, and for those respective samples the status thing that you are telecommuting he's giving us different different results.  Now, if I don't know population i'll consider it all correct.  But is the mean if I do reverse engineering is this mean 29 equal to 33.  No.  Right or 37 if you say that is also not equal to 33.  So the sample that you are going to work with may or may not give you the exact population.  Okay.  So to deal with this problem we have something called less central limit theorem.",
        "  keep us in the next slide we'll talk about that.  So sampling variations, once again we are talking about samples that we are collecting.  being very they keep changing.  And because we are using the sample data which keeps changing the underlying data may or may not give you the appropriate statistics.  Right, so we may get different different results.  If i'm using unbiased techniques, I may get any of these samples, that means the sample selection is a random.  Random service technically random selection happens.  So, if your sample and the leg sample is getting randomly selected the calculations, will also be and these results that you're seeing are going to be random So if I consider this as my data and.  This become my random variable.",
        "  Because the values are random each so it's it's also running.  Okay, one question to you or.  Whatever that we get a calibrations that we're talking about with respect to statistics.  What results, we will get.  If I say mean if I say median median median may not be a parameter.  Since that non parametric case.  So any statistic value that we talked about reasons or.  excuses or courses or right.  This mean.  yeah.  I love the data, what is the data type that we are going to.  put all the statistics that we talked about.",
        "  What is going to be the data.  descriptor of continuous.  discrete.  Are you sure.  123456 what is the average you know.  So when you're taking me in on when you're doing some calculations on some data, there is always chance at the answer will be like in a proper decimal format, so it may be continuous mostly.  It has to be continued.  Yes, yes, so.  Any statistic that you want to get.  It right, it will always be continuing, that is.  Again, another standard concept that you have to remember.",
        "  Okay.  it's a bigger type of your statistic is always continues, no.  Not district.  In this example, it is shown with discrete examples.  Okay. yeah.  know.  If this is continuous data.  The statistic that we are talking about is up continuously.  Okay.  And we are trying to find out.  One value.  Of this data.  hey.  we're interested in getting one value out of this entire continuous data.",
        "  Property i'm in the chance of getting.  That one value.  What does a chance.  zero.  Exactly.  good chance of getting exactly that one value will be zero.  Okay.  So, using all these concepts statisticians have thoroughly understood the pattern behavior of this data.  And what they have observed is.  That distribution of the sample status to talk about mania.  The distribution is all with normal.  Okay respective whether it is coming from data which is normal or not.  The results are always going to be normal, or approximately normal.",
        "  Right.  Now, if it is normal that means your status stick.  which we have to calculate is coming from some data, and we are seeing this data is normally distributed.  Okay, but there's continuous than it estimates.  That means it has certain properties.  As a variable is distributed normally with.  New and Sigma.  yeah.  So, considering these concepts, we are basically trying to make some studies here.  If I basically talk about first moment business decision, what is it talking about.",
        "  I mean it's nothing but me, but what does mean.  Speak above.  Average.  median or average and what does it.  Distribution distribution.  mean.  Okay variance is about distribution.  Yes, yeah I mean is the represent a to value or the characters.  Central tendency exactly.  So it is a representation or. characteristics.  Right and this data mean is representing about you, which gives some meaningful influence about your entire dataset.  Thank okay so that's that's actually what meaning is all about.  Now, when we are trying to calculate population parameter.",
        "  we're trying to calculate population parameter, and these population parameters right is like your.  Fixed reserved for them and fixed pattern.  expected value, but when we are trying to calculate this we are getting status take.  And we get multiple statistics, but each of the statistic was done or calculated in order to get parameter.  Right.  So when we speak about.  Identifying the statistics, then we are saying that the value is coming from normally distributed data.  And normally distributed data is.  Having these two properties.  characteristics.",
        "  So mean is the representation so for all the possible statistics that we are talking about becomes your random variable now, and this random variable is trying to represent parameter.  Thank you, a sample data is trying to represent.  parameter, so we are looking at.  we're looking at.  Average of average years.  If you do this average of average yes.  This is X bar one this expert to this is expert three so on so forth right if we do average of average us.  This is your expired.  Right.  Then we will get population average if I do average of whatever it is, I get new.",
        "  Because that's what this entire data is represented, we are interested in getting that only right.  So expired one X bar to so and so forth, if I tried to do that.  Again opposition.  This is the specific property that we have to remember, and this is what central limit theorem is speaking.  mean off means.  This is X bar mean off these expires is basically giving us population mean view.  At the same time standard deviation.  is basically nothing but Sigma by square root of its various basically Sigma squared y en.  Therefore calculations views that ugly variance.  Others, otherwise we cannot deal with the red different summation it becomes a zero.",
        "  Right so standard deviation.  X equals two Sigma squared Rudolph and which is the variation in these samples with respect to the Center.  So if I consider this a sample data and then variable.  Then first moment, which is the mean is going to be the mean of these results second moment is the variation among these samples, that is, can split that is calculated by square root of.  Right, so this is basically be.  Calculations for your random variable.  That random variable is nothing but the statistic.  Okay, the big days status it.  Is it making sense for you want.",
        "  Any questions, yes.  yeah any questions anyone.  i'm assuming everybody's aware it's understood.  So central limit theorem okay this theater.  is basically coming to a conclusion, saying that the distribution of the sample statistic right the distribution of sample statistics will be not.  Irrespective of whether the data that we are getting the population data that we're getting is normal or not.  Okay.",
        "  So that's basically your central limit theorem and the calculations for the random data that we're talking about is this first moment and second moment.  The second moment is also Congress standard error of me, because we are trying to calculate menier so we sent a standard deviation of me.  is nothing but the difference.  Here, that we are speaking about is the.  difference.  Okay, on an average, what is the edit that descended.  yeah now standard error is nothing but standard deviation on Defense is just that in standard deviation they are looking at individual data.  Basically, looking at X one X two X three so on so forth.",
        "  Whereas standard error is the average.  That we have calculated.  This is the red sample me so standard error is talking about variation in these means sample means with respect to.  very, very important central limit theorem studied try to digest it by how to remember it.  Great it's just like to table.  Okay.  Okay now.  Using this concept will try to implement a logic try to implement concept on not doing inferential statistics.  No.  yeah before that.  What does the probability of today's temperature being 20 degrees.",
        "  it's coordinates, at least in hyderabad it is cold.  So if I say to this temperature is 20 degrees, because it is continuous the probabilities equal to zero right.  right because it's a continuous skin temperature is a continuous to finding exactly one value.  Which is 20 degrees is zero.  Right.  So how about taking a range, which is minus 5200.  So what what we are basically saying is if this is your continuous access temperature could be.  20 degrees.  The same temperature could be 20 degrees.  yeah.",
        "  If it is 20 degrees.  Then getting exactly that value is zero.  But.  I can say that the temperature could be minus 50 degrees 200 degrees.  Right.  Can can be temperature tomorrow be between minus 50 degrees 200 degrees.  yeah.  Mobility still be zero, and I said, because it is a continuous can we cannot actually 10 that.  No, no, no that's what we calculated la area under the curve, it is not.  Okay okay Okay, so you had a another call concept comes up.  Exactly by by because i'm saying minus 5200 degrees.  Right okay.",
        "  So whenever there's a particular interval only then we can actually use this formula it's like because previously, you have just given up what are they going to value right like if it is about six at that we can qualify them for the exam so.  When you say.  about six add we were basically talking about the range, which is extended to succeed.  Okay okay.  Oh, there is a range here so area comes into picture.  Okay what's the least value will be take OK OK.  So it's like wherever only if there's a range specified, only then we can use this particular.  Otherwise, you can discover this.  Know whenever there is a continuous data.  Okay okay.  Then we look into this range calibration if it is discrete I know that there are probably fixed value.  Yes, yes, so it's like if they give like on your particular value like you know what is the probability of temperature being 20 degrees then it's like obviously zero but.",
        "  Like when they give a range anywhere if there's a continuous data and there's a range, we can estimate it using confidence interval.  hey yes area under the gun.  Yes, yes.  But.  Should I say minus 50 degrees 200 degrees tomorrow's temperature can be anywhere between minus 50 degrees 200 things.  Can I say that.  It is still difficult to estimate monster.  But that is the estimation and telling cheatham you're asking me what was the temperature going to be like, so I am saying that tomorrow it will be between minus 50 degrees 200 tickets.  anyways anywhere in that range, whatever the value that might be, it will be there between minus 50 degrees 200.  Okay, because it is temperature like a via because, obviously, like the temperature may not be like soul, or so high, but.",
        "  it's like if suppose it when you consider the concept of Marks like let's take on demand, so you trade if we can make, even if you say that you may score between at 200 marks also the probability maybe zero like I may not.  Maybe 200.  OK, so my question is now giving the example of marks, I want to predict what marks been stretched now score if I conduct an exam yes.  So I can say she can score anywhere between 02 hundred.  or 500 is a maximum and zero, and it is obviously between 02.  yeah so.  i'm trying to predict.  Okay okay you're asking me to predict MC rationale can score marks anywhere between 02 hundred degrees so 100 bucks.  Okay, what is difficult, like like you cannot say that 60 to 80 or 8200, then that will like still the probability will be zero in that case even.  During exactly, but my question again understand the question my question you see you're interested in learning.",
        "  You are asking me that if I write an exam now right i'm not sure, no, no i'm asking up but i'm saying that hey tomorrow, I have an exam, can you tell me well how much would I score.  Then, so what is that you're interested in, are you interested in telling me, I mean hearing that the answer could be anywhere between 02 hundred months.  know that is obvious right, you also know it, why do you need to ask anyone. Yes.  But then when you are asking this you want to get a precise value probably want to hear exact marks at marks you're going to get at marks for sure you want to listen, this number but probability of that single number is always zero if data is continuous.  Yes, yes okay.  So you cannot give a single number.  Okay.  The same time I cannot give the complete range.  Okay.",
        "  right if I say if my my day town lies between zero to one okay my total data lies between 02 right okay.  So, if this is my range.  What will be the area under the curve for this rich. Okay.  If you under the covers how much.  one.  Between the 100% obviously I know.  Yes, that record is going to score marks between 02 hundred that is for sure hundred percent.  He is.  Right, but I want to listen to more practically acceptable results.  So maybe.  I want, if i'm reading I want to hear that the Max could be somewhere between 70 to 80 or 80 to 90 or whatever, that is.",
        "  This portion is what you will be interested in. Yes, yes.  So the chance that you have.  Right to get marks should lie in this range. Okay.  Okay, I get it so.  The prediction that I am doing to you cannot be having such a wide range, because there is no point doing prediction is that.  it's covering the entire range.  Yes, yes, so that was my question like minus on it like.  Obviously, the maximum we between 02 hundred right.  yeah exactly that that's where i'm coming right, so the the prediction that you're trying to do, should be more closer.  To the actual that you.  That means.  That means that you're speaking about should reduce.  Okay, the rain should reduce it should get closer and closer to the single value per parameter.",
        "  But now.  If I say.  that's not has a chance of scoring anywhere from 30 to.  90.  i'm doing the prediction okay.  i'm saying that don't worry about right the exam you'll have a chance of scoring marks between 30 to 90.  Compared to.  i'm saying that you have a chance of scoring somewhere from night 75 to 90.  When I am trying to do the prediction.  or let's say you are doing the prediction to me.  Okay i'm asking you the question and you're coming up with the predictions.  option one is this option two is this.  In each of these two cases.  You have more confidence.  That when you.",
        "  have been different because it's only a range of 20 right so at least range, it has.  Like that's a more precise answer if you're looking for a precise answer.  But what does the chance that you're going to be correct.  The chance is very less chances with that because.  Your confidence is will come down a.  Yes, definitely yes, yes.  So Mike what my question was your confidence when you are trying to tell these results to your clients.  well.  Your confidence also reduces with the range.  Okay.  Right yes.  So you cannot be giving a full range wide range, at the same time, you cannot be giving a range, which is too short, where your confidence is going.  To come. Okay.  Right.  So you need to have.",
        "  A balanced approach.  Okay.  This calculations.  That we're trying to.  Look at is called as confidence interval because you're giving some interval.  And that interval should also be having some confidence associated with it.  So tomorrow's temperature could be 20 degrees to 40 degrees 10 degrees to 15 weeks.  Confidence in this result will be more.  Your confidence when you are predicting the results in wider range will be more because the chance of the actual value falling in this range is more.  isn't it if this is the area under the bell curve i'm trying bell curve, because these are continuous values that were speaking about.  Right alright, so this is continuous one yo.  If I see 20 degrees to 40 degrees.",
        "  Then tomorrow's temperature actually being in this range.  will be less right probability associated with this range is less. Yes.  But if I say 10 degrees to 50 degrees, the probability of the value falling in this range increases.  So that means the prediction that i'm doing has more chance of being correct.  Right, hence it scholars confidence interval.  So let us see how how to actually do the calculation and how do we actually interpret this.  So a bank with hundred thousand customers.  There is a bank financial institution, and it has hundred thousand customers.  And they're offering.",
        "  That thinking to offer credit card to its customers.  Okay they're planning to give credit cards.  But giving credit card means there is a lot of investment also involved isn't it.  And the any business right in this case, credit card business the profitability of this option or this business opportunity is going to be.  Depending on what is the total balance that the customers are maintaining because credit cards, if you talk about credits credits are given.  That, if you have money, it has a bank if Bank does not have money, how can it give credits.  How does bank to get money by the savings account by the deposits.",
        "  that the individual customers are aggregated all the result will be the capital investment for being the giving the credit cards or credit systems.  So profitability of the card.  will depend on how much money.  Does that bank has that money is based on the average balance that the card holders are make the customers are Nick.  Right.  Know should I go ahead and launch the credit card design business head and deciding to come up with this option should I directly launched the credit card land for everybody.  What hadn't been blessed yeah not every.  Now, we should not blindly invest in any business, not just credit card business any business idea, you have should you directly jump in invest money.",
        "  Or do you do preliminary analysis, research on whether the idea will work or.  Definitely will do research before.  Implementing anything.  Exactly hey so any investment that you have to do you have to be careful.  So blindly investing and then assessing whether it will work or not is a foolish decision, you should not be doing that.  What if it doesn't work all your investment is gone for us isn't it.  So you're going to look out for.  The option on should I invest.  For that, first, you need to do an assessment of the idea here the credit card launches an idea and we are trying to research assistant.  To do that, we are trying to do a pilot lunch.  We generally do pilot lunches.",
        "  Much market research campaign is launched, and then we using that right credit Karma banks set up these.  cans, the temporary.  What do you want the stalls and they try to read give pamphlets and try to ask customers about different things, and all that so on the similar lines, a market campaigns run.  And people who are interested, for them, these credit cards will keep.  So 140 customers accept the car.  right when they were launching in the marketing campaign probably 500 different customers came up, but only 140 customers accepted among all the customers that back.  Okay, so this becomes your pilot launch you're given this credit card to their customers and whenever you have credit card.  Right.",
        "  If you invest in current if you have a credit card, we always try to use the credit card.  right we don't directly use our bank account.  Savings account debit cards.  Right, we will use credit cards.  So that means the average balance.  That we maintain will be a little higher.  Right, so we are trying to see.  What is the average balance.  That the customers these 140 customers are going to maintain because profitability is directly proportionate to the balance that right so market research campaign is launched in which about 140 customers accept the card as a pilot.  Now, when we invested this.",
        "  date when we invested the on on these campaign launch the idea is to gather the data and then assess this is your sample.  They were doing inferential statistics here, this is sample.  But as this is your population.  OK.  So the data that we gathered is like the average balance maintained by these 140 is 90 $90 the standard deviation so much and we're also saying assume the population standard is so.  Now, given the sample data.  We have to identify how much would be the average balance if the card is given to all the customers hundred thousand customers, what will be the average fan.  Okay let's let's see what what all things we have and what is that would be to calibrate.",
        "  So i'm writing the notation yes symbols hundred thousand is your population size.  sample size is one for.  Right now, this 19 $90 that we are looking at is actually represented as X bar.  This is X bar.  Right, because they are talking about these 140 customers it's a sample data right so expert then $2,833.  This is represented as smallest standard deviation of the sampling.  assume that the population standard deviation population standard deviation is Sigma.  assume that the population standard deviation is.  2005 that done.",
        "  So these values are given to us and we say what can we say about average balance that will be held after a full fledged market.  That meets this is the full fledge market lunch, what will be the average balance for this, so it is population parameter, and this is what we need to identify.  What is our view.  yeah.  Difference what will be the mean.  Should I say.  it's been inferential statistics.  We will be.  Like we should know like Mina.  And to be liked by Mr Sam more than.  Later yeah. Okay.",
        "  So, are you saying that we will be doing multiple market lunches.  Like at least, we need the data like only one samples me, maybe I think we cannot get the population.  No, no, no, no, you will be always working with one sample only.  See practically do you want to launch this multiple times.  know you.  Basically, you want more data.  yeah but you cannot just doing this, you cannot just keep doing this and not take a business decision.  Any scenario, you will only be working with single sample next.  yeah so let me.  Talk about this now.  See we're talking about central limit theorem.  Central limit theorem says that.",
        "  The statistic value that we are get actually doing the calculation for a price calculation is a.  Continuous value the statistic.  is always a continuous map and if it is continuous, it will be normally distributed and if it is normally distributed a certain properties. Okay.  And we are interested in finding out a single value of this normally distributed data.  But when I tried to do this in the value calculation, it will be zero property would be zero right hands, we take range, but if I take range, I cannot take the full range I have we appreciate you taking the rich, these are the discussions that we have right.  Now let me ask you a very simple question okay.",
        "  you're you're traveling to offices right probably you have started traveling to offices Now the question is how much time.  Will you take to travel in.  Time to travel from.  Your home to office.  yeah but, what is your answer is.  going to be in some ways, like 20 minutes to 30 minutes, if not the participants depending on.  Depending on that traffic so you're saying 20 to 30 okay.  yeah there's.  Everybody will.  might want to write.  share your answer.  It also depends on a mode of travel right like if you're going by metro and probably take less time away.  Whatever it is.",
        "  i'm not worried about all that i'm just asking you to tell me what is the time travel that you basically.  It would be an arranger Nice are obviously so 40 minutes to 60 minutes 40 minutes to 60.  Okay you're saying 40 to 60 okay.  Assuming you're traveling only by one mode of transport.  The respect to whatever.  Oh, my number two one enough or.  If I have to convert that you two.  minutes, then I would probably say 60 minutes to 90 minutes.  That do you do 40 somebody saying 30 to 40 Okay, my question to you all.  Right 60 to 90 minutes or why 20 to 30 minutes or white 30 to 40 or 40 to 60 days for in your respective cases, why not this 25 to. 30.  like this.  With.",
        "  10 to nine, irrespective of the case i'm talking about let's say, if you are cases in your office or home or fixed distances right I don't think it will change.  The word for permission has different distance, for me it is.  done i'm not talking more pernicious i'm talking only about your case you said 20 to 30.  So, for your case i'm asking why 20 to 30 is.  Your answer, why not 10 to 14 or why not 25 to 30.  Why do you do take 10 minutes interval.  Why not 30 minute interval or why not five minutes.  minimum and maximum time like I would have ever taken the least amount of ever taken to travel, what is 20 minutes in the maximum time I would have taken in wants to travel with is 13.  Now, how.  Are you conclusion that.  My question is, how are you, including.  Are you sure, basically, whatever the title travel time that you had to know the experience that till now is your sample data.  Yes, yes.",
        "  Are you sure you're going not going beyond 30 minutes in future.  populations.  We can see this more probability of the time being, between 20 to 30 minutes what.  Is that is a.  yeah what is what up there is an accident on the road.  yeah.  Yes.  Then it will vary.  Yes, sample data from your past experiences only they're talking about all.  This, but I want to also consider the.  confidence that that you are coming up with this scenario. Right.  If you say 25 to 30 minutes, maybe.  There are there will be cases where you're going to reach beyond I mean you're going to take beyond 20 minutes sorry.  30 minutes.  Sorry, my office.",
        "  Several guys, he said, the as the rage increases, then the confidence levels also increased when the range is smaller than the confidence levels also will be lesser right.  yeah that's what i'm trying to explain, yes, if I see 25 to 30 million you.  can say 25 to 30 minutes.  There are chances where you might reach office, the late.  More than 30 minutes are there might be cases where you're going to reach office early before 25 minutes.  Right so chance of going wrong will increase.  Right, whereas here the chance of going wrong is less.  Compared to this, the chance of going wrong is less here right.  So whatever we are we're basically talking about it, so if you're saying 20 to 30 minutes, I mean i'm only talking about one example Okay, you said 20 to 30.  This could be 25 and we basically just 25 plus or minus five.",
        "  Right hey.  it's in the first place, why 20 to 30, why not some other than.  This is because of your.  Experience sample data.  And this is with respect to your Conference.  If you want to be more conflict say i'm your manager and if i'm asking you this question.  Right, you need to give more chance to yourself, so that you reach that office begin the time right, so in that case you probably say 10 minutes.  Right, because the scope that you are getting now will increase.  She will be more confident when you are representing this number isn't it, she will say 15 minutes to 35 minutes, whereas if you say 25 plus or minus five minutes, that means you're only given 20 minutes 30 minutes range for yourself.",
        "  Okay.  yeah. Everyone making sense right.  Yes.  it's making sense of.  confidence.  30 confidence.  That, if the range is more like ranges more white than on the ball.  Obviously, yes, because the chance of your value falling in that range increases.  That, but if they go the minimum and maximum mistaken I Canna marks case you have begun like 02 hundred then there it's be like hundred percent confidence if the complete range is taken.  Yes, yeah.",
        "  So there is no chance you're going wrong, say, the actual value can live between this marks. Yes.  So there is. Some confidence, maybe.  that's what i'm saying.  Okay.  So when the actual result comes let's look at this so i'm basically saying 02 hundred 200 bucks you write the exam you get tourism you'll get some dessert.  Right.  That is what we were trying to predict in the initial case.  Yes, now that result will be in this range on the right, yes, so that means whatever I predicted is correct.  But if I see.  let's say 50 marks to say 50 to 80 bucks.",
        "  Okay, my range is now 50 to 80 bucks.  If this is the case when you write the exam there is always a chance that you have got more marks, whatever it is, at point one, two or 85 or 90 whatever, but there is still a chance that you might score more marks.  Right, are there is a chance that you might get less than 15 months.  If you have scored a mark, which is like 85 is my prediction correct.  No definitely so.  I will not have the same confidence, where I have at this stage.  02 hundred percent i'll have 100% confident hundred percent sure that the result is.  going to fall industry, but here it is not.",
        "  that's that's what you're trying to understand now just imagine whatever i'm talking about just imagine on a range like this, this is your continuous scale okay X axis i'm saying 02 hundred.  That means you're all the values will fall here on.  Correct.  area under the curve is hundred percent one.  So any result that you're calculating will be in this accepted region hundred percent region.  But if I see.  50 to 80.  So 52 let's say at.  This blue lines would be my range.  And if the actual value falls within this range, only then i'm correct my prediction is that the left side and writes it portion.  Is.",
        "  The chance, where you're going wrong, these are if these values are falling on your mark is falling, with this then your prediction is wrong.  Right.  This so the chance, you have the confidence you have we diminish that chance or confidence is measured using this area.  i'm just trying to bring up the same point again and again and again and again, because you need to understand the concept here.  Now now now the coming back to this scenario credit cards in there you'll see the visualization also if you see this scenario.  Can I see based on my sample.  A single sample that we have taken.  which takes 1990.  Right.  If we consider this as the result.  Then.",
        "  This may be not hundred percent correct.  Why because.  If my underlying sample very let's say.  assume you're not doing this, but assume if you have that.  Under samples in a different location, then your data would have been changed.  Based on the locality, you might have a different set of customers.  Right coming into the marketing campaign and different set of 140 people might come into picture right.  In those different people are coming, then their bank balances will be different.  And their usage will be different, so do you not get the same number isn't it.  So there is a lot of uncertainty, because your samples can be.",
        "  Right, so we don't consider or we don't trust, a single calculation also single point if I do the calculation does.  Property Z so considering both these concepts, we are coming up with a range.  As an answer that range is called us confidence interval.  That confidence interval basically is calculated us point estimate plus or minus margin margin of error.  Point estimate.  Is your sample statistic.  sample statistic margin of error is the variation that we have within that.  sounds I mean if I have in the central limit theorem we discussed all this right.  If I have multiple sample statistic sample means.",
        "  Then first movement of this random variable.  will be population parameter, but if I look into the second moment.  Second moment is the variation in this data with respect to him is calculated Sigma squared off.  Okay scary Sigma pi squared.  Right, but again, if I further expand this.  The variations can basically influence the confidence Okay, so the margin of error that we are talking about is associated with the confidence level.  Get the score less confidence level.  Confidence values.",
        "  So X bar is your point estimate.  Right and that X bar.  is also getting married by so much.  The zoom we are taking Z because we are talking about statistic being right normally distributed so we're comparing it with the values are.  So, to get the range we're looking at zebra.  Now, this one minus alpha that you're seeing is called us confidence level.  Right and alpha is called us significance live.  i'm going various you know i'm trying to bring out all those right aspects that you that are mandatory for us.  To understand any questions, please ask me.  Do not hesitate minority.",
        "  So alpha is representing significance level, this is nothing by.  The error push.  chance off.  era.  The industry right in athletics stream or in business scenarios.  Whenever we want to try to do the predictions.  Right, the predictions are, by default, always our band, assuming that we are 95% correct.  Okay.  So this is called as 95% confidence level if I use that 95% confidence level, I will get confidence interval.  That means, if this is my.",
        "  Go bunker i'm saying that i'm 95% correct.  me this is the portion where your data force.  You still have chance of going wrong, but that is very little.  Right, so now, what is this value if I know that on Z, then the calibration can that.  So how do we get this value.  To look at this I have X bar 19 $90 I have Sigma 2833 This is one for T squared off and small sample sample sizes 140.  Right.  This is the portion where I need to do the math now and to get to as well, if I can get that, then I can calculate the confidence.  So industrial standard is.  95%, so we will assume that apartment is our first.",
        " 95.  So that means I know the area under the curve.  But what are these values which gives us that area is something that we have to find out.  about the rain would be very wide.  By.  Like if we take 95% like it's like predicting the mass will be between 20 and 99.  Because you're talking about distribution of the data we don't know what our values are you fall.  Yes, right so range may or may not be that big we don't know, and it was the shape of the car remember the normal distribution shape of the car is influencing all these things, what if your shape is like this.  yeah this is Max assume this is your marks.",
        "  yeah yes, so in that case only limited little portion of data is going to fall in the stadium majority is going to fall here.  But what if I have.  This case.  Then yeah you can see that right.  Good amount of data falling in this significance region as well.  So it's all about the distribution of the data.  Okay.  So. Coming back to this discussion.  Reno know area under the curve.  Okay, we know the area under the curve.  But what we want, we want this values right if it is Z then i'll put Z here.  have to be with us to be.",
        "  Z the Center will always be zero so this becomes minus Z because Z is standard normal distribution, which is perfectly symmetric and if this is zero this is he then this minus this will be minus end it is symmetric shape right, so it has to be minus.  So that's about your.  confidence, but if I consider this this shaded portion is 95%.  That means 0.9%.  Right.  Now I want to identify what is this the values, using the tables, because in the table, we have the calculations already ready.  In the previous example on the gmat scores.",
        "  We knew X and from that we calculated Z then we refer to the table to get the probability or area.  But now I have this area or probability.  And I have to move backwards, so I will get Z using the table.  hey, but if you talk about the table the table will contain all the values from minimum to that particular batch.  Right so here in order to identify this number, we will take the left side.  Blue shutters.  So this is what we are going to take.  The blue shaded portion.",
        "  What will be that shaded portion crusaded portion if I have to try it and more a good image, this is the image when she image.  were saying that 95% confidence right so let's say this is.  This is 90% okay.  So that means from here to here it is 95. yeah.  I need to know what will be this see this will be minus eight, this is, you know if I know this, I will be able to also know this because it is classy.  yeah so Z table will always capture.  The values which are from.  left side value right it's always calculating the portion from lower minimum to that particular well.  isn't it.",
        "  So what is this portion.  This is 95% So what will be this portion rates.  mean minus four minus at my age standard deviation.  whoa whoa whoa whoa no i'm talking about area under the curve shader ocean.  Exactly. that's good.  2.5% why because they yeah and that discovers 95 that means total will be remaining 5% but Z symmetric so that 5% is getting further divided this is.  To banter.  This is corners alpha by two alpha by two.  Okay, and this portion is called as one minus and.  This is one man is that.",
        "  If this is 2.5 percentage if I convert this into decimals what will be the result 0.0 to five right for.  developer hundred I get 0.0.  Correct.  So we will go to.  The table.  I will search for the value, which is.  This shaded portion which is 0.025 that gives me dizzy by.  So if I look around the this table where will I have 0.025.  I see some number, but this is 0.00 to five, that is not the value that we want, we want 0.025.",
        "  This is occurring when my value is minus 1.96.  yeah that means.  This portion that we're talking about is.  minus 1.968 if I remove this this becomes plus 1.96 and this becomes minus 1.6.  Right. This is minus 1.96.  This is plus 1.6 is this clear to everyone.",
        "  Not.  Everyone is done from is this clear. TV.  Okay, I want you people to now.  Do the calculation for.  90% and 99% also.  Do the calibration ways for 90% first.",
        "  done.  what's up guys.",
        "  That means it's not clear for you people still. 1.644 90% 31.6 190.  Okay, very good.",
        "  Point 074 9931 07.  So, can you please like once again explain, for 95% i'm just trying to figure out for 95.  see whether it is 90% 95% or 99% we have.  The area.  Okay, under the bell curve ignore the area 90% in this example, it is 90% okay now I need to know what what Z values.  We are getting this portion as 90%.  Okay, where this is symmetrical curve, so it will be plus or minus Z.  We are trying to figure out what is that plus or minus diva you.  Okay.",
        "  So for that, if I have to refer to the table the table always calculates only the left side portion of your bell shaped curve.  Facility for calculations.  Okay, so that means if I need to know what is this minus Z.  Then I need to know what does this say that portion in the first.  Okay, if this is 90%.  Okay, this will be.  The outside is 10%.  yeah.  This is your alpha.  90% is one minus alpha so alpha is 910 percent.  Right, but this is only alpha by two this is alpha right.  Right yes.",
        "  This is alpha by two that means it's 5% 5% means in decimals what is the value 0.0505.  So i'm going to refer to the 0.05 in the Z table.  Where is my 0.0 0.05 0.05.  Sorry, I do not have exactly 0.05, but I see that there is five units additional five units less.  And that value is occurring at 1.64 at this Center averages average of these two.  So this is going to give us plus or minus 1.64. Okay. OK so.  now use the same logic try to do it for 99%.",
        "  i'm getting my supervisor so. Could we.  It is 99% 99.  This becomes 1% that means this because point 5% That means there is another zero here.  Are you looking at values with 0.005.  Okay, so what is the value that you're getting 2.57 2.507. How sweet.  2.5.  Okay.  But it's not 0.005 right exactly. yeah.",
        "  So this episode minus 2.575.  Average of 70 and. 82.757.  Now this is plus or minus not just my best, because you might also want to see that right side, so does exactly cemetery so this busyness.  That so we see the. data.  yeah if you want to consider to the left side, we have to consider it as minus and to the right side, we have to consider it as plus yes. Yes.  So this on to this 2157.",
        "  This value is something that you have to buy heart plus or minus 1.96.  Okay.  Now let us go to the calculation.  Calculations don't mind just a very simple calculation all these values are given.  Right.  So if I do Sigma squared, and this is that is that accurate in average is this the confidence interval or you can also call it as interval estimate is equal to.  Point estimate plus or minus margin of error.",
        "  Point estimate is the statistic that you have categorical in our case, it is expert plus or minus margin of error margin of error is basically taken a Z it's one minus alpha into Sigma squared off in.  Red one minus alpha is called as confidence level, this is.  main different presented this to to have to do that.  The Z value is plus or minus 1.96 here we are saying.  1990 X bar.  plus or minus Z has one minus alpha.  To summarize what six.  Sigma by Square and to 11.2 main.  comma plus 1.96 into cooler and cooler.  This is the range.",
        "  If it has to be 90% confidence, confidence is decreasing right so.  Your range will also decrease 1990 plus plus or minus my by.  This is minus.  1.645 into to 11.29 comma 1990 plus 1.645 into to learn.  This 6.1 foreign minus plus or minus 1.96 that we see that is the only value, which is changing in both these things and based on that change the range of luxury to.  Get.  your questions.",
        "  Okay i'm assuming no questions.  Like, we have to do this only after making the data into normal later.  Absolutely.  Data has to be normal.  So we have to use the appropriate transformation technique and then make it normal and then apply this.  In Python.  Okay, yes, yes, yes.  Now, whatever the languages that are a SAS based tool, whether it is our whether it's by them or any other tool that we, we probably might want to use this is the logic that you have to data has to be normal in order to interpret all this.  location right, yes, so we will take a small break now.  it's a 440.",
        "  will connect at 448 eight minutes.  not going under the break to another so yeah eight minutes just just come back in eight minutes guys.  So there does seem to see.  Someone oh.  yeah good good.  So in this case, actually, we want the population standard deviation but impractical impractical as like after getting like we after doing some one for the customer, so we can get a standard standard deviation for one last question oh so.",
        "  yeah that that's the next concept punish will discuss that with me.  Good good.  Right analysis.  that's one of the important point that we have to discuss.  Okay, so before we go to that let's talk about the things that we discuss now.  So confidence interval is basically nothing but that interval that we have to come up with for predictions.  And this is basically done on let's say point estimate plus or minus the error that that we have because of the sampling variation.  We call it a margin of it.  So point estimate is nothing but the calculation that will be doing, because we will always work with a single sample on will not work with multiple templates or a single jump.  So point estimate plus or minus margin of error will vary, based on the confidence that we are trying to print.",
        "  Because of Z at one minus alpha.  into the variation with among the multiple samples that might have.  So that variation is called as a standard error, this is called a standard error.  Is not standard deviation standard deviation speaks about.  The variation in one sample.  That variation of the individual records, with respect to the mean, but here standard error, we are trying to interpret the variation among the surface multiple sound.  Good, this is the formula.",
        "  Now, when we do the calculations, the confidence level is something that we have to measure it has nothing but the points on the Z axis, which give us the area under the curve.  So if I know that area under the curve being 95%.  Then, what is this Z that's what we're trying to identify minus n plus.  So when we do the calculation before that, let me record this Z value at 95% is plus or minus 1.960 90% is plus or minus 1.645.  And zero 99% is plus or minus 2.575.  Now these three standard values or something that you might want to remember.  Okay, especially this.",
        "  That the default 190 5% confidence is the default confidence level that we need to measure that predictions.  Right.  Now.  Then we do the calculation at 95% confidence right, this is the results that the calibration expert assignments, we get this.  night, but if you look at those results, the statements that we pass it.  Those statements should include all the references that we are basically using.  The mean violence of the population nice industry.  mean balance right off the population license the strange very good, so we are talking about meal lying in this range, but what confidence.  Is it 90% 95%.",
        "  That is not explained by so confidence level is not given here, so this cannot be here right state.  mean balance is in this range 95% of the time, so this is given.  But mean balance is it new or is it X bar.  That is not mentioned, so this is also not.  We have to refer to the sample also and we are making the statement because using that sample only we are getting this stuff, and this is completely wrong because you cannot use only 95% of the customers, you have to use 100% of the data.  So the only to corrupt interpretations, at least.  and basically we are saying.",
        "  mean of the population has 95% chance of being in this range for a random sample That means, if you take any random sample.  And do the calculations, the mean of the population has 95% chance of being in this range.  Right I.  mean of the population will be in this day and mean of the population will be in this range.  right for 95% of the run up so that means if I take hundred samples.  Then 95% of the samples X bar values will lie in this range God.  And that's that's what the second statement So these are the only two statements which are correct, not these three.",
        "  So, ultimately, we are saying that this is the population result parameter, but that is obtained using sample statistic and at certain confidentially.  So if you use all these three, then these are corrupt interpretations.  Now the actual question counts the formula.  Confidence interval is X bar plus on my eyes, I mean margin of error rate, so it is a point estimate plus or minus margin of error.  The margin of error is nothing but.  see it when when is i'll find to standard error.  standard is nothing but Sigma squared.  Now this formula is used to calculate or estimate parameter.  admins on population data, what will be the results.",
        "  But this is done in the first place, because we only have sample data we don't have population.  The population data is not available then how can I get this parent.  it's rather a I cannot have a parameter used to calculate another parameter because, if I am able to get this parameter, then I should be able to be also get this parameter right I don't need to do all these big calculations not required.  Right.  So practically speaking.  You will not have Sigma.  theta if the bankers new one, hence the population standard deviation from the previous lunches is not reuse, that means I don't know what was the previous lunch average.  There is seek my is not.  What are we doing.  We are calculating new using expert point St.",
        "  hey if I don't know Sigma can I also not use sample standard deviation to estimate population standard deviation.  that's what we are doing in the first place right.  We are trying to take statistic to interpret for parameter.  Correct, so we are trying to do that heroes, not just on the meal, but also send division.  So the best the replacement for Sigma is its finest.  But if I have to do the calculation, we are considering the variation aspect is what.  I get our data to take X bar because it's but keeps changing isn't it, the reason why we have to see equation confidence interval equation.  Then, how can I blindly estimate Sigma using yes here also there will be variation right.",
        "  That variation also should be captured.  If I have to use small, as in the equation, and a variation that comes with this Sigma is also something that we have to consider.  Right.  So to capture that variation.  That is under distribution called T distribution, and this, the distribution is used.  Okay, p distribution is just.  A distribution is also known as student tea distribution.  Which is also branch, you can say of normal distribution date same normal distribution concept.",
        "  hey so rather than Z distribution now, we will have to use the distribution, which also accounts to be variation of the Sigma.  variation calculating same.  Right so that's.  The key distribution usage.  T distribution, as I mentioned, is just like a standard.  has one property.  It is very similar to see that one difference is basically the category property of your peers.  And that is.  This property.  As then how the sample size increases that T distribution tends to become Z distribution.  So D distribution will also get affected with the sample size, which is also expressed as degrees of freedom.",
        "  And minus one.  minus to read those degrees of freedom to be used.  So, if your degrees of freedom is one that means your sample size is too.  Then.  This becomes your Z this becomes your.  Right Z.  You can see.  That there's a clear case of difference.  Right.  Different clear differences.  But as the sample size increases.  As the sample size keep increasing that gap between these two is reduced, and at this stage it's almost equal.  But they are not going to be exactly.  Right.",
        "  So this is.  N minus 130 degrees and almost to touch that's your property.  They will not be exactly the same, but there will be very, very close.  If n is tending to become infinity.  And is increasing sample sizes increasing that means if sample size increases the variation is that using zenda.  So your tea distribution tends to become Z distribution.  See very, very simple, if this is my population if I take a small sample then there could be a lot of variation between the sand.  Right between the values that will not have a.  But if my sample size increases, then that variation between the samples will be there.  Right.",
        "  And the distribution is all about capturing that way.  So as Z as n tends to infinity T tends to becomes.  And a formula is changing like this.  In place of Sigma we are replacing.  smallest.  Okay.  But as to counter this variations we bring in T distribution in place of CDs.  Right, and here we have one minus alpha.  And N minus one as well because T distribution is getting impacted by the sanford's.  Right.",
        "  Now I know what does expire, I know what is smallest, I know what is square root of n I don't know the tea distribution T values.  So just like how we have the distribution.  We also have P table just replace that Z with the that's a.  that's your details.  right hand this is your table.  And yet this is the table.  The table, you have to look at the first column.  which has degrees of freedom.",
        "  And at the bottom, you see the confidence level, Sir, I mean that's how we read it at the top, this is like one to two tails at all that you can ignore it.  What it means we only look at left side who tells me boo the Bush okay.  So that's basically your. calculations.  And the year we want.  to capture.  The P value for one minus Alpha and N minus one.  One minus alpha is.  95% N minus one small island is 140 so this becomes 113.  But if you look at the table here.  We don't have one for.  The next closest value is hundred.  Okay, so we will take 95% at hand.",
        "  And it's a physical page, so we cannot enjoy everything, but when we do the calculations you'll also get these exact values us.  So 1.984.  Alright, so that's your calculations, so we replace that P value with. This calculation.  And this is the numbers that you will see.  So this is smallest 2833 right, and then you look at look at this and T value at 0.975 or 95%.  is one that 1.988 degrees of freedom as 130.  Okay don't get confused with 0.975.  If this is your curve.",
        "  If this is 95%.  And then, this will be 2.5% So if I had both of them i'll get this he left said cumulative portion right for 97.5%.  This is the Z value.  Get 95.7 verse.  hey, this is the.  positive side, if you do have been this point, then you might want to write it as 0.025, and this will be one minus.  minus 1.0 point nine.  So when you do the calculation.",
        "  very similar to the interpretation that we have done for the table same thing here on fertility.  All right.  Any questions.  yeah guys any questions.  nope so.  Now these things that we have done manually right let's do that.  In Python.",
        "  So go on, let me down on the business basic Statistics Code that you have already have read and go all the way down.  Standard edition normalization and after normal qq blood yeah this is busy distribution characteristics.  Alright, so this is my data i'm importing side by dot stats.  This is required because the Z distribution is calculated using this tax functionality so stats dot nom dot CDF CFS cumulative distribution function.  It basically takes the X value.  there's the syntax OK.  Serious CDF X are your you can also call it a squad time comma new cameras.",
        "  But this is the representation parameter representation, so we have six at mujer 711 segments 20.  Right cumulative distribution function.  If I execute this it will give me the area under the Left curve on the left side push that is your CD of cumulative.  Distribution function.  All you need to passes mean it standard.  So this 14.25%, to be precise.  Then we have point percent function or percentage point function.  So I know the percentage.  That I know the percentage.  This is 0.025 I know this percentage.",
        "  What is the point at which i'll get this person.  and passing meal and Sigma for Z value status.  Z has the standard right, even if we don't give by default you're assuming that the function as your status 01 Sigma.  So we're talking about 0.0.  So point 4% point function is giving us minus 1.96.  Now this is what Z distribution normal distribution.  same calculation, if I have to do the area under the curve, then I will be doing the distribution for CDF RT distribution for percent point function.  Right So these are.",
        "  The calculations stats dot T dot CDF but here i'm passing.  The right value.  The value with degrees of freedom.  Alright So here we have to do.  quantel value right so i'm passing that one time value, I am getting area under the.  Great.  same way if you see ppm I know this value what will be the I mean this is the area under the curve and what will be the exact P value at which we get.  One minus 1.977 when very precise number calculations and.  So this is the.  Calculations that that basically are done.",
        "  Yet all these way we're doing it in man.  yeah so that's that's about your.  theory of doing predictions on numeric data inferential statistics, so when you are doing inferential statistics, remember that the samples might vary, hence we have to consider that variation and then probability of single number is equal to zero hands, we have predicted in a REACH.  And that's where confidence in the world coming to the picture.  and confidence interval basically says point estimate plus or minus margin of error and the margin of error is associated with the confidence level.",
        "  The range it which right the range, at which the results will fall under the bell curve, which is infinity skin we're trying to find out a range, in which the actual values might work for that becomes your competency.  Okay, so that's your inferential statistics and using these inferential statistics we will basically take decisions.  That informed decisions.  For example, number in the credit card scenario.  Right.  Should I launched the credit card program or not to all my customers, something that you have to take a decision.  So when you're looking at your investment and all that.  Right.  As from a business perspective okay from from a business perspective let's say you're you're looking at.  Some let's see.",
        "  let's see.  yeah $2,000 is the calculation that you are getting for Roi.  But whatever the investment that you, as you come in this video.  If your balance is going to be 2000 or more for all the hundred thousand customers, this is your population right if this was the piece.  hey, this is the estimation, so you are doing the calculations.  So your business is very clear that you need to get $2,000 but unless until you do the business will not know whether you will get $2,000 or.  Hence we do the.  sample calculations that by legend.",
        "  And within the pilot launch we get confidence interval we're trying to estimate the result so we'll get some range at 95% confidence if I have to take the default one.  If my right reserved confidence interval that i'm getting is basically saying.  1800 to 1900.  K let's say this is your range.  If you are in since.  As part of the calculations right pilot lunch.  yeah if i'm getting this should I continue with the business.  Should we continue with the business case.",
        "  depends on the credit card limit.  already mentioned be made of it then on investment is calculated as.  $2,000 which basically means that the minimum balance that all these hundred thousand customers should maintain 2000, this is the guideline the businesses deciding based on all the calculations on the investment that they have made, and all that.  From a business perspective management is clear, they need to get $2,000 to make business profitable.  than.  That i'm getting only 1800 and 1900 so should I continue my business.  Know obviously no definitely why because this condition is not satisfied.  Linda.",
        "  If my business is 1900 to 2100 I mean if this is the confidence interval the time it gets to this case one this is case should I continue, or should I stop.  may continue and consider to continue.  you're not launching are all the customers, maybe.  This will you have lunch, the population.  that's the objective you cannot say I only lunch for so many know.  it's what all the customers we're generalizing.  Then, maybe know because, like there's a chance of 1900 also so not meeting $2,000.  Okay, Peter completed, we.  should start from 2000.  Okay, yes, Sir 1900 to 2020 2100 basically is nothing but.  This is less than buddha's.",
        "  Okay, see if you're doing a business right you're investing thousand dollars you're doing some tasks and you're exactly getting thousand bucks in returns is this a good business model.  No.  No right, it has to be greater, it has to be greater than what you have.  invested on it, then you proceed.  So 2000 is the return on investment racing if I get $2,000, then the investment is returned.  I need more than 2000 to become profitable, so you are going to only look into the case if you are range is going to start from 2001 to probably 2005 or 2500 whatever that number.  Only if you're identifying.  Your confidence interval being like this, compared to 2000 only in this case, you will proceed with the investment and and proceed with the business.",
        "  Here we are doing.  Our informed decisions you taking a informed decisions and this informed decisions are also known as data driven decisions.  Rather than blindly saying okay this area, the business will be very good this business ideas very good let's invest.  Right, rather than that blind hand, these are decisions we basically come up with this data driven decisions are informed.  Right good.  Is it clear yes.  Yes.  Yes, sir.",
        " Okay.  This concept that we just discussed right is also called as hypothesis testing.  Information based on data.  Now this hypothesis testing is not being focused in the interview, so we also have remote from our regular. sessions.  at once upon a time, is this concepts were considered for interviews now they're not asking anything from this particular questions, so I only cover the theory particle theory, but then we'll move on to the next.  So.  You can just continue with recording you can do the practice yes.",
        "  So before like, as we have unwritten the numerical kind of production kind of thing like I have adults are like in assignments, in addition, three assignments actually have to predict the company's date of sales, like the output is a numeric.  Decision tree the desert has to be.  Here, because it has to be taken, but like no matter like how many times the accuracies like it's a big plus.  That means, be a bad idea tuning has to be tried more.  It has nothing to do with the classification or numeric data, it is all about the parameters that you're trying.  So that is also numeric data right like continuous numeric data than white on please conference.  automatically your confidence interval is calculated.  Okay, by default, like in those packets like they shouldn't be.",
        "  Any regrets at model, the calculation that you are doing, will, by default, assume 95% confidence, and you are going to actually get confidence interval.  Okay, to the calculations will not give you this interval in certain cases, the intervals, are also can what you can at this point estimate only.  Okay, so, in those cases like invasion to eat better like internally is calculating all those things that conference and we're not.  Even take the parameters, the confidence interval is also that yes.  that's it's.  very, very simple guys if my data is numeric.  Right and if it is especially continuous.  Because my underlying sample changes.  That and and the length sample changes.  The result so to capture that variation.",
        "  We use confidence interval.  Right.  Okay, and if the prediction that that you're getting is a numeric prediction the numerical but then the model is coldness regression model or aggressors if the model is giving non numerical but then it is called, as a class that's yes.  Okay next.  hypothesis testing, as I explained is all about.  Making certain assumptions for parameters.  But because parameters cannot be identified, we will take sample.  From population, and then we calculate statistics.  And while making estimations we use confidence interval for estimating the path.",
        "  and using the estimated.  Estimated results.  We will take business decisions.  But when you're trying to do this.  There might be cases where.  You may go.  Okay, so there are certain terminologies or certain standard process that you have to follow, let us try and understand that process.  Next.  let's it the business that we are speaking right was having an Roi as 2000.  Then. i'll put it this way okay.",
        "  So first case right, if I have to take a business decision on whether to launch credit card offer to be and then credit cards to the customer service.  lines credit packs Okay, this is the.  Right, so if I have to launch credit card and i'm basically taking a decision okay.  What would be your.  First.  Decision a.  Safer decision yeah, this is the question.  What will be your safer decision.  Any new change that you're bringing in there's nothing but a business challenge.  Right.  So you can score.",
        "  37 score. Similar.  Basically score is coming, what is the question you are the manager of the Bank.  mean you're the head of the Bank.  You have a decision to be taken, what is the decision should I launched the credit card to the customers or not. Okay. yeah.  If this is the key. Is.  Then, what will be your answer.  So this is, it is exactly like $2,000 is the return on investment, like we don't know, we need to consider the plus and minus cases also right so it's exactly the same value, there may be a minus case or so so.  It should, no, no, no, no, no okay again okay.",
        "  hold on hold on we're not talking about.  Our our you.  know our role is a condition see as a manager, as the.  head of the band, you have to take a decision but you're saying that Okay, if I have to be implemented, this is the investment that I have to put.  If I get only this kind of returns, only then it will be profitable that these kind of decisions you're looking at already I mean sorry this kind of information you're also looking at.  As part of that I said.  If the business gets.  Minimum $2,000, then the money is right or at it.  From that aspect of return that forget about the question here, all you have to think about us there is a business decision that you need to take should I launched the credit card to the customers are not.  So, as we like previously discussed, probably the launch a pilot program like will will choose very few people and launch it.  Is that is the process that you are implementing.",
        "  To get the data and then taken informed decisions but. Before that i'm saying.  Maybe if you have a decision.  You have to consider risk risk factor select even be.  plan Basra on look at all those things guys take simple steps.  My question is, will you launch the credit card or not, what two options, you have.  There will be two options only that.  If certain parts are all going to come when you're going to detail into that.  I ultimately for this question what two answers you have there will be only two answers, yes, you launch no you don't look.  Now, getting me.  Given this scenario.  Which one would you prefer as a risk less scenarios.  are giving them, obviously we choose to launch the credit.  Is that riskless.",
        "  feud launch that there may be losses also if you launch it so.  Exactly the riskiest rate.  so to speak, is walking.  See whatever you're currently doing you're assuming that that is a good scenario that is a good status that you are in all conditions are good.  That is the default assumption.  We also call it us status Q.  Right.  Now, what is your default or current condition for the business, no, it does not have credit cards it's just running the regular business bank is running the regular banking business, there is no credit cards.  So this will be your default assumption.  Okay.  Do not learn, so my bad.",
        "  you're going to say, do not know.  This is also called as no action decision or no action condition.  This is called is null.  hypothesis hypothesis means.  assumptions right.  How about this as means assumption, so the thing null hypothesis.  Now means no X you're not doing anything.  If you don't do anything that that means your current condition is assumed to be good right or default condition of current current.  So this is the not launch case.  Okay.  But there is a business decision that we need to take should I launch the credit card or not, if that question comes riskless.",
        "  Decision is go with your current conditions that means don't do something different.  But if you can find an evidence which says that if you launch credit card you're going to get benefited it basically means.  If you take action.  Right find evidence which says you take action and you get profitable in that case, obviously launch.  Right.  So the first condition is this in that business scenario, the alternate condition will be launched the grid.  So it's called as alternate hypothesis.  Take action condition alternate hypothesis So here we say launch, because that is the action that we are trying to evaluate.",
        "  But in order to come to this, we need evidence.  That evidence is what we were trying to calculate it the previous exam.  So we need parameter details, but we not get parameter details so we'll take.  Care sample and then calculated statistics.  But based on right the decisions have to be taken a parameter, so we will first make these statements or make those conditions.  Using parameters Okay, then we'll take sample than statistics, then using statistics.  We will try to estimate parameters.  and using that estimated parameters take decisions.  Okay.  No null hypothesis represent rise, which not H zero.",
        "  alternate hypothesis represented as hitched a or you can also say which one.  In a hypothesis an alternate hypothesis.  Okay.  But.  The decisions are to be based on certain conditions in this example that condition is taken from this Roi assumption that we made right, we were assuming that this was the calculation that was done in $2,000 was the return on investment mark.  So when you do this calculation.  you'll get estimated parameters.  range right interval.",
        "  So using that interval we will take a decision, so the statement that we will have the conditions that will right here for null hypothesis is that if, on an average parameter remember the statements have to be a parameter if on an average.  The conditions are that I will get or the business will get less than or equal to.  $2,000.  hey.  As a as the voltage right average balance.  Then we will say.  Not much no action and probably say.  That means you're not launching them.  But if you find out an evidence which says the calculations are going to give you greater than $2,000, then you will take.  Care guys.  We are framing the conditions we are framing those scenarios.",
        "  quantifying those and then using the calculations, using the results from population sample.  We will.  Try to conclude on what decision we have to take in a business condition.  Clear.  Yes, sir.  yeah we're still kind of looking at university to.  Be.  If certain parts are always dead, if this condition is changing of that condition is changing we'll deal with that, but for now, this is basically a simple example of your hypothesis testing, which basically allows us to do the.  inferential statistics for taking a business decision. Right.  Now.  We have seen confusion matrix right, I mean the cost table.  isn't it.  it's a two by two table that we have see.",
        "  Those similar lines.  Here also, we have a two by two table.  Okay, and that two by two table basically gives us insights on kind of decisions.  and wrong decisions.  Very simple.  If this is the condition.  I should not launch the business right, I mean I should not launch the credit isn't it.  If I come to know that.  My balance is going to be less than $2,000 which was like return on investment condition.  Then I should not be launching my business right.  isn't it.  But in this case, if I go ahead and take action, what happens.  yeah in this case, what will happen this.",
        "  What happens.  Is this not clear.  Is it clear or not clear, the question.  So, can you repeat the questions. up.  So same business scenario.  Okay now.  The return on investment was saying $232,000 is what the average balance should be made by each customer.",
        "  When you did the calculation you found that the business right is not generating balance with the customers are not maintaining the balance which is greater than.  balances less than or equal to.  So in this case I should not be launching my credit card right.  Yes.  But if we take action, take action to launch in this case, it is only two decisions if I take action, then what will happen in this case.  going to Los yes.  We are going into losses.  Similarly, I come to know that.  The balance is going to be greater than $2,000 that the people are maintaining.  But rather than taking action i'm saying i'm too lazy not to take action.  In this case, water.  We just remain where we are there's no growth.  Exactly the chance of.",
        "  Becoming profitable is.  is reduced, we don't have the chance.  Right.  So there are correct decisions are you may also take the wrong decision.  Correct, so there are two by two tables basically here that that we can create for options or net.  And that two by two table can be explained like this.  If you are null hypothesis is true that means, if you would have launched your credit card business, you would have gone into losses that's the condition that means your balance would have been less than or equal to $2,000 imagine, this would be the case, this was the case in real.  Ideally, you have to.  frame to reject the null condition fail to reject means you are your don't have you're basically not having evidence.",
        "  To do something different from what you are doing, currently.  Good that scholars fail to reject the.  null hypothesis, the current condition right.  you're doing that anyways.  If you have evidence, you can change that condition, but here we are saying I don't have evidence to change.  Right, when this is the scenario, you will go with null hypothesis for me, this is the right decision this is represented as confidence.  that the value is lying in that confidence region.  But if.  But if, in this case, if you reject the null hypothesis that you're saying that no I will go ahead and take business decision and launch the credit card.  If this is the scenario, then it is going to be wrong decision isn't it.",
        "  This is wrong decision.  We call this particular decision is Type one error, and this is represented as and far representation.  Okay.  This is one scenario now as Jim you're going to get greater than $2,000 that means you're going to get profits actually.  But you're too lazy to take action.  You fail to reject null hypothesis you're saying that no i'm too lazy I don't believe this a lot want to launch.  The credit card so you're doing whatever is the current condition you're repeating.  This is Type two error.  represented us beta.  This is also wrong decision.",
        "  And finally, you have the right decision here.  It says see if it is greater than 2000 you go ahead and launch it.  reject your current condition and do something different.  With this is.  nothing but alternatives.  So this is called as power of the test because, ultimately, this scenario is helping us to do something different from what we are doing, but take some action.  And it's called as power of the test.  Now Type one and Type two, these are the names case there is no other game, the errors are called has Type one and Type two.  Type one error is dangerous because you're inviting lawsuits.  You should not be taking any action you should be risk free but here you're getting into this you're inviting this.  Right.",
        "  Basically, a type one error is a condition, where you're taking action when you should not be.  Taking action when.  not required.  Or you can say.  you're rejecting.  It.  Through null hypothesis, this is your Type one represented with and.  This is not taking action just exactly opposite to not taking.  Not taking action.  when required.  Not is coming here.  that's your type do, or you can say you're failing to reject a false null hypothesis.  cradle to reject.  falls null hypothesis.",
        "  Okay, this is called as Type two error, which is represent recipe.  I found it clear.  Okay now.  Based on what we are trying to estimate the value that we are trying to estimate right there are different tests that we have to perform.  What we have seen that the Z test and the T test that we have seen is.  For continuous case, your why the output that you're trying to predict, if it is continuous then we follow these right flow charts.  And we discussed with respect to.  This.  Z test.",
        "  On Peters, this is what we discussed when Sigma was known we go through here Sigma is not known we go for.  Similarly, we have various other test cases.  Based on different conditions, but all these different test cases are.  For bias continuous case.  Similarly, we will also have.  The test cases which are applied for.  discrete cases as well, but discrete cases are always easier to calculate.  Okay, remember that.  You don't have complex complication when your output as discrete data, you simply take proportion as.  Well, what proportion to proportion of high school.  So these are basically different options of your inferential statistics, and these are not limited only to these, but you have a lot of other use cases also.",
        "  But always remember when it is continuous, you will have a chat.  Okay, various various various things have to be verified normality test has to be checked right.  All those conditions have to be verified and if you follow that flow chart you will probably be able to.  Go indie film.  All right now for detailed discussion of these hypothesis testing, you can refer to the recommended videos.  But yeah, as I mentioned, theoretically, the subject is more important because in interviews more advanced things that we focus now, these things are not being asked.  Right yeah.  So this is about your.  hypothesis testing or inferential statistics.",
        "  This concepts that we discussed right these concepts that we discussed here, especially your confidence interval nclb its central limit theorem.  will form the base for doing the predictions, especially when you have output, which is American.  In our next, yes, we will start with regression is.  Now.  So what exactly means regression if i'm I continue of output as continuous then.  or put logic, we are to apply, and then we also have variations in regressions some of them are not so important, but yeah we still have those techniques.  Right, and this is more like I but unique concept.  And these are some of the advanced regression techniques that we have, which are not used anymore in the industry from statistical standpoint, we have those concepts.",
        "  But was the regression, we will move to time series, and finally, we will go to black box or will do black box and then boogie times.  So that's the future classes next week's guess we'll start with regression analysis.  Mark your attendance now.  yeah brushing know.  So I have a doubt related to the hierarchical clustering so if you'll, allow me to, so the so my screen then i'll so because, in theory, the input, it is not an issue, because I have a doubt there are so many.  After hold on let everybody finished the attendance part, then we will. populate.  yeah please confirm once the attendance part is done everyone.",
        "  So we have class next week so.  What happened next.  gen first.  Oh okay i'm.  Dan first I don't think there is any heavy planning on.  It we.  have not done.  Okay yeah.  31st is locked down.  let's confirm so 31st and first logged on.  i'm i'm not sure i'm not a party person mean hearing that 31st.  There will be no movement outside everything will be logged in so again, assuming that is the case okay.  So when can we come to officer.  Like for the classes offline class.",
        "  yeah I mean today, nobody will be there in office, and yesterday it was on holiday so cleaning and all those things will not, and that is the reason today's classes offline or online next week, we will see you for our officers, a Min we have our first then we'll we'll kind of go in office.  Okay, so I seem to be having class.  I will confirm i'm not sure about.  Okay, so okay so so please inform like if it is offline class.  yeah i'll mention that both things.  Please very much preferable select event.  So you and I also see listening.  Now I mean I understand a distance so we'll we'll try to have it offline.  Okay, so.  That would do this, there was no problem, but I mean I would have come, but the challenge was with respect to the office, it would have not been cleaned and all that so probably, we will not be able to stay.  Okay, Sir celebrations, and all those things happen on okay. that's Nice.",
        " yeah.  i'm done so Okay, thank you.  So.  Okay.  Everyone please confirm or change the screen now.  Okay i'm changing the screen assuming everybody's done don't don't come back and say I did not do it.  Alright, so let's change. The stone.  yeah.  No shadows.  Its.",
        "  east, west airlines data.  Okay. Hello.  There is a column called smiles and then because, as far as I am, you know i've been gone through this that I, there is a zero, and then the value is 2024, so it is kind of a kind of these outlets are so how can I deal with.  The first.  date of the water you think would be appropriate, did you understand what a square miles means.  Why is.  ready.  Yes, a place.  Where my.  countenance polycom for top right.  Now, what does that mean.",
        "  Number of miles countered as qualifying top flight miles right So what does that actually mean did you understand that first.  Most likely that is a concern.  Right you.  Know see the point is no, you have to do a thorough understanding of the business conditions and the data.  If you would have understood what does square miles means.  Though the data description is given, but did you research on that.  Did you try to ask a question on why is 2024 value their wise $500 value and suddenly by zero.  If you would have done that art read that research, then.  You should not have this.  question in the first place, you should be able to take a decision.  it's all about understanding the data that part is not done properly.  Business understanding data understanding are going to be very, very to vote there was you not know what to do, or to do.",
        "  But before that i'll have competing.  Yes, I said, it is not only.  Not only just you know put in a data into the Python and then do the analysis as we usually do.  And after that we'll get the clusters and whatever the yes, it is not like, but before that i'll have to understand you know, business and then the that given this column wise, what are the variables.  So that is why.  Because things.  Do not every information.  question.  itself, there is something.  Which is expected from your side on so.  That person will have to do research try and understand that business scenario try and understand what those columns me.  That planning.  part is see.",
        " Every.  Once you get the data doing in Python as one asked what what to do, why to do something that you have to research and dried find out so that part is missing. Okay.  Apart from this.  or two incidences fine yeah.  No, no i'm losing you again, there is a.  Can you hear me.  I can hear you Hello yeah yeah.  Now I can hear whether.  There is a one column in.  The date column, so how can I deal with this date column date.",
        "  you're asking me the answer that.  New research.  yeah. OK.  OK, I will okay very good.  And then.  Once the computer okay fine yeah.  If you're if you're done research, and if you still start then.  yeah I can help you but.  without doing your part.  I have.  literally just gone through the columns and all that we.  think you.  should try it out.  Let me.  Then i'll get the next session.  I think.",
        "  It is the most important part, I think, because I met.  My.  First start tuning this but insulated material, I thought you know what i'm saying.  Not but yeah practice you're going in the right way.  Small.  focus on that one partner.  You should be able to.  Clean clean these things.  Okay okay that's good.  Okay anyone else any other questions anyone stuck at any point.  One thing one more.",
        "  Time boundaries and to submit all the assignments.  No, no there's no time.  Because being a working person, it is you know, either because i'm you know.  yeah I understand that's that's the reason we're saying no law deadlines.  But the condition is.  Unless until you finish your assignments and submit them, you will not be eligible for project.  Oh.  yeah.  Okay, all right so we'll wind up and.  I have one question can.  I be burning the petition we go for it, there is no values to be go go for implementation or.",
        "  If there are outliers in particular corner, so we should go for media implementation eight.  Okay, yes yeah.  So, so how how it is, first, we should like my question is, first, we should go for the implementation on first we should go for it out loud treatment, and then I.  guess there is.  There is no.  Fixed procedure there Okay, you can try it out due to either of these things.  see it again boils down to the question on.  Why outlier what outlier why missing values that the variance right So if you if you look at those cases, you will be able to take a decision on saying that Okay, I have three the outlier first and then we probably will do the calculation of 11 calculation for invitation.  it's a logical decision, and there is no standard rule that you have to do this faster than. OK OK OK.",
        " mostly by typing for speech and go for it go for the invitation hello, can you hear me.  yeah I can hear you but.  I don't know i'm not sure where you have redacted there's no standard like that okay so it's.  it's like everything.  isn't it is it.  No it's the individual's choice, of course, which is getting influenced by the data and the business conditions okay good Thank you, Sir.  Okay, then let's wind up and reconnect next week."
    ],
    "Topics": [
        "'CRISP-DM', 'Crisp ML(Q)', 'Feature Engineering', 'Feature Engineering'",
        "'Data Mining', 'Data Cleaning', 'CRISP-DM'",
        "'Labeled Data', 'Tranining Data', 'Classification'",
        "'Multivariate Analysis', 'Factor Analysis', ' Unsupervised Learning'",
        "'Classification', 'Regression', 'Machine Learning'",
        "'Ensemble - Sampling', 'Deep Q-Learning', 'Model Selection'",
        "'Ensemble - Cross Validation', 'Steps for Forecasting', 'Holdout Sample'",
        "'Model Evaluation', 'oracle cloud infrastructure', 'Model Selection'",
        "'data nuances', 'Steps to Train', 'Survival analysis applications'",
        "'Data Science', 'oracle cloud infrastructure', 'Feature engineering'",
        "'Business Analytics', 'data nuances', 'Video analytics'",
        "'Bagging and Random Forest', 'Bagging', 'Boosting Models'",
        "'Boosting', 'Ensemble Gradient Boosting', 'Boosting Models'",
        "'Continuous Variable', 'Gaussian naive Bayes', 'Discretization'",
        "'Gaussian naive Bayes', '\\xa0Contractive Autoencoder', 'Maxima and Minima'",
        "'Multi-Class Classification', 'Mean', 'False Positive'",
        "'Prediction accuracy', 'True Negative', 'Error Function'",
        "'True Positive', 'True Negative', 'Prediction accuracy'",
        "'Frequentist Statistics', 'True Negative', 'Prediction accuracy'",
        "'Normalization', 'Data Transformations For Better Normal Distribution', 'Gaussian naive Bayes'",
        "'Gaussian naive Bayes', 'Normal Distribution', 'Missing completely at random'",
        "'Steps for Forecasting', 'Prediction accuracy', 'Naive Bayes'",
        "'Machine Learning', 'Tranining Data', ' Machine Learning'",
        "'Positive skew', 'biased sampling', 'Bias-Variance Trade-off'",
        "'Model Selection', 'Mean'",
        "'Hot Deck Imputation', 'pooling layer', 'A goodness-of-fit test for multinomial logistic regression'",
        "'Test Data', 'Mean', 'A goodness-of-fit test for multinomial logistic regression'",
        "'K-means Clustering', 'K-Means', 'Density-Based Clustering Algorithms'",
        "'Adam', 'Adam Optimization', 'Centered Moving Average'",
        "",
        "'Variable Deletion', 'Mode Imputation', 'Median Imputation'",
        "'Naive Bayes', 'Avoid overfitting', 'Model Selection'",
        "'Polynomial Regression', 'Normal Distribution'",
        "'Error function ', 'image interpolation', 'Base Equation'",
        "'Data Transformations For Better Normal Distribution', '\\xa0Undercomplete Autoencoder', 'Error function '",
        "'Normalization', 'Standardization', 'Data Transformations For Better Normal Distribution'",
        "'Standardization', 'Z-test', 'Median Imputation'",
        "'Mode Imputation', 'Derivatives in Optimization', 'Error Functions'",
        "'Gaussian naive Bayes', 'Standard error', 'Normalization'",
        "'Error function '",
        "'Error function '",
        "'Error Functions', 'Vanishing Gradient', 'Log Loss'",
        "'Trimming Outlier Technique', 'Error Function'",
        "'P-value', 'Z-test', 'P-Value'",
        "'Adam', 'Mode Imputation', 'Error function '",
        "'Standardization', 'Pairwise deletion', 'Feature Engineering', 'Feature Engineering'",
        "'Mode', ' Range', 'SVM'",
        "'A goodness-of-fit test for multinomial logistic regression', 'Positive skew', 'Log Loss'",
        "'Normal Distribution', 'Gaussian naive Bayes', 'Model Assumptions'",
        "'Variance', 'Coefficient of determination', 'Pairwise deletion'",
        "'Optimization'",
        "",
        "",
        "' Range', 'Support in Association rule', 'Mean'",
        "",
        "'Bold Driver'",
        "'Avoid overfitting', 'Trimming Outlier Technique', 'Dimensionality Reduction'",
        "'Challenges in Gradient Descent Algorithms'",
        "'text normalization', 'TEXT SUMMERIZATION', 'Lowercase'",
        "'Normalization', 'Standardization', 'Dimensionality Reduction'",
        "'Normalization', 'Standardization', 'regularization'",
        "'Cost Function', 'Exponential Decay', 'Polynomial Regression'",
        "'Probability', 'Probability', 'Probability'",
        "'Gaussian naive Bayes', 'Model Assumptions', 'Probabity Distribution'",
        "'Predictor Variable', 'Multinomial naive Bayes', 'Tranining Data'",
        "'Binomial Distribution', 'Negative Binomial Distribution', 'Binomial Distribution'",
        "'Frequentist Statistics', 'Binomial Distribution', 'Negative Binomial Distribution'",
        "'Frequentist Statistics', '1 Proportion test', 'simple random sampling '",
        "' Range', 'Mean', 'Median'",
        "'1 Proportion test', 'Inferential Statistics', 'Frequentist Statistics'",
        "'Test Data', '1 Proportion test', 'Named entity recognition\\xa0'",
        "'Response Variable', 'Descriptive Statistics', 'Big Data'",
        "' Range', 'NLU', 'Mode'",
        "'Frequentist Statistics', 'Response Variable', 'Random Oversampling:'",
        "'Random Oversampling:', 'Random Undersampling', 'Unbiased sampling'",
        "'Random Oversampling:', 'Random Undersampling', '2 Sample t Test'",
        "'simple random sampling ', 'Unbiased sampling', 'biased sampling'",
        "'Descriptive Statistics', 'Mean', 'A goodness-of-fit test for multinomial logistic regression'",
        "'Gaussian naive Bayes', 'Continous Data ', 'Mode'",
        "'Continous Data ', 'Continuous Variable', 'Gaussian naive Bayes'",
        "'Missing completely at random', 'Missing at random (MAR) ', 'Bias-Variance Trade-off'",
        "'Gaussian naive Bayes', 'Descriptive Statistics', 'Probabity Distribution'",
        "'Central Tendency', 'Descriptive Statistics', 'Median'",
        "'Mode', ' Range', 'Descriptive Statistics'",
        "'Mean', 'Descriptive Statistics', 'intra-class variance'",
        "'Coefficient of determination', 'r squared and adjusted r squared', 'Standard Deviation'",
        "'Standard Deviation', 'Standard error', 'Variance'",
        "'Gaussian naive Bayes', 'Probability Distribution', 'Inferential Statistics'",
        "'Standard error', 'Standard Deviation', 'Variance'",
        "'Standard error', 'Mean error', 'Coefficient of determination'",
        "'Continous Data ', 'Continuous Variable', 'Conditional Probability'",
        "",
        "'Continous Data ', 'linear Interpolation', 'Interval data'",
        "",
        "'Market Mix Modeling', 'lasso and ridge regression'",
        "",
        "' Range', 'Goodness of Fit'",
        "'Polynomial Regression', 'Prediction accuracy', 'Maximum Likelihood Estimation'",
        "'Prediction accuracy', 'True Positive', 'True Negative'",
        "'Bias-Variance Trade-off', 'Lift In association rule', 'Prediction accuracy'",
        "'Confidence Interval', 'IQR', 'Polynomial Regression'",
        "'Probability', 'Probability', 'Probability'",
        "",
        "'Survival analysis applications'",
        "'Hypothesis', 'Type I error'",
        "'Market Basket Analysis'",
        "",
        "'Market Mix Modeling', 'Confidence Interval', 'Frequentist Statistics'",
        "'Standard Deviation', 'Standard error', '1 Proportion test'",
        "'Market Mix Modeling', 'T-Test', 'Market Basket Analysis'",
        "'Market Mix Modeling', 'Multinomial naive Bayes', 'Cross Sectional Data'",
        "'Continous Data ', 'Probability Distribution', 'Probabity Distribution'",
        "",
        "",
        "",
        "'Confidence Interval', 'Probability', 'Precision and Recall'",
        "",
        "",
        "'Precision and Recall', 'Prediction accuracy', 'Bias-Variance Trade-off'",
        "'False Positive', 'Prediction accuracy', 'True Negative'",
        "'Prediction accuracy', 'Probability', 'True Positive'",
        "'Error function ', 'Mean error'",
        "'False Positive', 'False Negative', 'True Negative'",
        "'Support in Association rule', 'Confidence Interval', 'Frequentist Statistics'",
        "'Mean Imputation', 'Mean error'",
        "'Standard Deviation', 'Bias-Variance Trade-off', 'Descriptive Statistics'",
        "'r squared and adjusted r squared', 'Bias-Variance Trade-off', 'Missing at random (MAR) '",
        "'Prediction accuracy', 'Mean error', 'Confidence Interval'",
        "'Bias-Variance Trade-off', 'Coefficient of determination', 'Standard error'",
        "'Polynomial Regression', 'variance', 'Goodness of Fit'",
        "'Data Transformations For Better Normal Distribution', 'Polynomial Regression', 'Bias-Variance Trade-off'",
        "'Data Transformations For Better Normal Distribution', 'Median Imputation', 'Median'",
        "'Backpropogation'",
        "'Down-sampling', 'Image enhancement', '\\xa0Effect anti-aliasing filter\\xa0'",
        "'Mode Imputation', 'Adam', 'Negative skew'",
        "'Trimming Outlier Technique', 'Down-sampling', 'Error function '",
        "'Mode Imputation', 'Stemming', 'Non Maximum Suppression'",
        "'noise reduction', 'Down-sampling', 'VGG-16'",
        "",
        "",
        "",
        "'Polynomial Regression', 'Error function ', 'Cost Function'",
        "'Adam', 'Adam', 'Centered Moving Average'",
        "'Hyperparameter', 'Hyperplane', 'PageRank'",
        "",
        "'Adam', 'Confidence Interval', 'Cost Function'",
        "'Mode Imputation', 'Mean error', 'Adam'",
        "'False Negative', 'False Positive', 'Mode Imputation'",
        "'Normalization', 'Mode Imputation', 'Data Transformation'",
        "'Standard error'",
        "'Bias-Variance Trade-off', 'Cost Function', 'Precision and Recall'",
        "'Standard error', 'Standard Deviation', 'Mean error'",
        "'F-Score', 'Polynomial Regression', 'Coefficient of determination'",
        "'Mean error', 'Prediction accuracy', 'Precision and Recall'",
        "'Bias-Variance Trade-off', 'Imbalance data', 'False Positive'",
        "'simple random sampling ', 'Degree of Freedom', ' Range'",
        "'Confidence Interval', 'Standard error', 'Missing at random (MAR) '",
        "'Ensemble - Sampling', '1 Proportion test', 'Random Oversampling:'",
        "'Standard Deviation', 'Standard error', 'Data Transformations For Better Normal Distribution'",
        "'Variational Autoencoder', 'Data Transformations For Better Normal Distribution', 'Standard Deviation'",
        "'Cross Entropy', 'Variational Autoencoder', 'Response Variable'",
        "'Degree of Freedom', 'A goodness-of-fit test for multinomial logistic regression', '2 Sample t Test'",
        "'Pooling Layer', '1 Proportion test', 'Winsorization Outlier Technique'",
        "'Data Transformations For Better Normal Distribution', 'Single Exponential Smoothing', 'Dimensionality Reduction'",
        "'Data Transformations For Better Normal Distribution'",
        "'Variable Deletion', 'Lift In association rule', 'Listwise deletion'",
        "'Error function '",
        "'Negative skew'",
        "'Text Similarity'",
        "'Normal Distribution', 'Data Transformations For Better Normal Distribution', 'Obtaining a Multinomial logistic regression'",
        "'Probabity Distribution', 'Probability Distribution'",
        "'Standard error', 'F-Score', 'Error Functions'",
        "'Error function ', 'Variance', 'F-Score'",
        "'Missing at random (MAR) ', 'Prediction accuracy', 'Precision and Recall'",
        "'Confidence Interval', 'ROC-AUC', 'Obtaining a Multinomial logistic regression'",
        "'Confidence Interval', 'Market Mix Modeling', 'Mode'",
        "'Lift In association rule', 'Confidence Interval', 'Bias-Variance Trade-off'",
        "'Buisness constraint'",
        "",
        "'Buisness objective ', 'oracle cloud infrastructure'",
        "'Business Intelligence', 'oracle cloud infrastructure', 'Business Analytics'",
        "'Hypothesis Testing', 'Hypothesis', 'Frequentist Statistics'",
        "'False Positive', 'True Negative', 'Na\u00efve forcasting'",
        "'Mean error', 'Bias-Variance Trade-off', 'Confidence Interval'",
        "'Na\u00efve forcasting', 'Frequentist Statistics', 'Predictor Variable'",
        "'Cost Function', 'Error Function', 'oracle cloud infrastructure'",
        "",
        "'False Positive', 'False Negative', 'confidence in association rule'",
        "",
        "'Model Selection', 'Obtaining a Multinomial logistic regression'",
        "'Decay Parameter Challenges'",
        "'Type II error', 'Hypothesis', 'Type I error'",
        "'Hypothesis', 'Conditional Probability', 'Bayes Theorem'",
        "'A goodness-of-fit test for multinomial logistic regression', '1 Proportion test', 'Model Selection'",
        "'Hypothesis', 'Type II error', 'Type I error'",
        "'A goodness-of-fit test for multinomial logistic regression', 'Type II error', 'Features of survival analysis'",
        "'A goodness-of-fit test for multinomial logistic regression', 'Frequentist Statistics', 'Market Mix Modeling'",
        "'Survival analysis applications', 'lasso and ridge regression', 'Association Rule'",
        "",
        "'Power rule', 'oracle cloud infrastructure'",
        "'False Negative', 'Type II error', 'True Negative'",
        "'Type II error', 'Type I error', 'Type I error and Type II error'",
        "'Type II error', 'Type I error', 'Type I error and Type II error'",
        "'Type I error', 'Type II error', 'Type I error and Type II error'",
        "'Type II error', 'Type I error and Type II error', 'Type I error'",
        "'Goodness of Fit', 'A goodness-of-fit test for multinomial logistic regression', 'Test Data'",
        "'A goodness-of-fit test for multinomial logistic regression', 'Bayes Theorem', '1 Proportion test'",
        "'Qualitative analysis', 'Hypothesis Testing', 'A goodness-of-fit test for multinomial logistic regression'",
        "'Regression Analysis', 'Transformations', 'Equation of a straight line'",
        "'Ensemble - Stacking', 'Forecasting methods', 'regression improvement transformation'",
        "",
        "",
        "",
        "'Mathametical Foundations for Data Science'",
        "'Hyperplane', 'Market Mix Modeling'",
        "'Business Analytics', 'Cross Sectional Data', 'data nuances'",
        "'Missing Data', 'Listwise deletion', 'Holdout Sample'",
        "'Topic modelling ALGORITHAMS', 'CRISP-DM', 'Crisp ML(Q)'",
        "'Outlier Analysis Treatment'",
        "",
        "'Outlier Analysis Treatment', 'Polynomial Regression', 'Avoid overfitting'",
        "'Speech Recognization  Deep Learning model', 'Speech Recognization Preprocessing'"
    ],
    "Link": [
        "'https://www.datascience-pm.com/crisp-dm-2/', 'https://ml-ops.org/content/crisp-ml', 'https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114', 'https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114'",
        "'https://www.tibco.com/reference-center/what-is-data-mining', 'https://www.tableau.com/learn/articles/what-is-data-cleaning', 'https://www.datascience-pm.com/crisp-dm-2/'",
        "'https://blog.cloudera.com/learning-with-limited-labeled-data/', 'https://www.techopedia.com/definition/33181/training-data', 'https://www.merriam-webster.com/dictionary/classification'",
        "'https://en.wikipedia.org/wiki/Multivariate_statistics', 'https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/', 'https://www.javatpoint.com/unsupervised-machine-learning'",
        "'https://www.merriam-webster.com/dictionary/classification', 'https://en.wikipedia.org/wiki/Regression_analysis', 'https://medium.com/@lizziedotdev/lets-talk-about-machine-learning-ddca914e9dd1'",
        "'https://www.researchgate.net/publication/317061892_Ensemble_Sampling', 'https://www.geeksforgeeks.org/deep-q-learning/', 'https://en.wikipedia.org/wiki/Model_selection'",
        "'https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/', 'https://medium.com/analytics-vidhya/time-series-forecasting-step-by-step-a06dac2293ff', 'https://www.statistics.com/glossary/hold-out-sample/'",
        "'https://www.saedsayad.com/model_evaluation.htm', 'https://www.oracle.com/in/data-science/', 'https://en.wikipedia.org/wiki/Model_selection'",
        "'https://www.nuance.com/omni-channel-customer-engagement/analytics-solutions.html', 'https://developers.google.com/machine-learning/gan/training', 'https://odsc.medium.com/when-to-use-survival-analysis-applications-in-industry-data-science-d8bfb7566f43'",
        "'https://en.wikipedia.org/wiki/Data_science', 'https://www.oracle.com/in/data-science/', 'https://www.analyticsvidhya.com/blog/2021/04/a-guide-to-feature-engineering-in-nlp/'",
        "'https://www.researchgate.net/figure/Business-intelligence-business-analytics-breakdown-Klimberg-Miori-2010_fig1_276491531', 'https://www.nuance.com/omni-channel-customer-engagement/analytics-solutions.html', 'https://viso.ai/computer-vision/video-analytics-ultimate-overview/'",
        "'https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/', 'https://www.ibm.com/cloud/learn/bagging', 'https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/'",
        "'https://en.wikipedia.org/wiki/Boosting_machine_learning', 'https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/', 'https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/'",
        "'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://en.wikipedia.org/wiki/Discretization'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://iq.opengenus.org/types-of-autoencoder/', 'https://www.geeksforgeeks.org/application-of-derivative-maxima-and-minima-mathematics/'",
        "'https://medium.com/analytics-vidhya/ml06-intro-to-multi-class-classification-e61eb7492ffd', 'https://danielmiessler.com/blog/difference-median-mean/', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://360digitmg.com/loss-functions-in-machine-learning'",
        "'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1'",
        "'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1'",
        "'https://www.techopedia.com/definition/1221/normalization', 'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://en.wikipedia.org/wiki/Normal_distribution', 'https://stefvanbuuren.name/fimd/sec-MCAR.html'",
        "'https://medium.com/analytics-vidhya/time-series-forecasting-step-by-step-a06dac2293ff', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier'",
        "'https://medium.com/@lizziedotdev/lets-talk-about-machine-learning-ddca914e9dd1', 'https://www.techopedia.com/definition/33181/training-data', 'https://en.wikipedia.org/wiki/Machine_learning'",
        "'https://en.wikipedia.org/wiki/Skewness', 'https://www.researchgate.net/figure/Illustration-of-Sampling-Bias_fig2_342587109', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229'",
        "'https://en.wikipedia.org/wiki/Model_selection', 'https://danielmiessler.com/blog/difference-median-mean/'",
        "'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3130338/', 'https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/', 'https://pubmed.ncbi.nlm.nih.gov/17156271/'",
        "'https://www.opencodez.com/software-testing/test-data.htm', 'https://danielmiessler.com/blog/difference-median-mean/', 'https://pubmed.ncbi.nlm.nih.gov/17156271/'",
        "'https://en.wikipedia.org/wiki/K-means_clustering', 'https://en.wikipedia.org/wiki/K-means_clustering', 'https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/'",
        "'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.', 'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/', 'https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/'",
        "",
        "'https://methods.sagepub.com/reference/encyclopedia-of-measurement-and-statistics/n473.xml', 'https://statisticsglobe.com/mode-imputation/', 'https://medium.com/analytics-vidhya/feature-engineering-part-1-mean-median-imputation-761043b95379'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html', 'https://en.wikipedia.org/wiki/Model_selection'",
        "'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://en.wikipedia.org/wiki/Normal_distribution'",
        "'https://en.wikipedia.org/wiki/Error_function', 'https://www.di.univr.it/documenti/OccorrenzaIns/matdid/matdid358544.pdf', 'https://www.cuemath.com/algebra/linear-equations/'",
        "'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://iq.opengenus.org/types-of-autoencoder/', 'https://en.wikipedia.org/wiki/Error_function'",
        "'https://www.techopedia.com/definition/1221/normalization', 'https://support.industry.siemens.com/cs/document/109759445/standardization-as-the-basis-of-digitalization-?dti=0&lc=en-WW', 'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9'",
        "'https://support.industry.siemens.com/cs/document/109759445/standardization-as-the-basis-of-digitalization-?dti=0&lc=en-WW', 'https://en.wikipedia.org/wiki/Z-test', 'https://medium.com/analytics-vidhya/feature-engineering-part-1-mean-median-imputation-761043b95379'",
        "'https://statisticsglobe.com/mode-imputation/', 'https://tutorial.math.lamar.edu/classes/calci/optimization.aspx', 'https://360digitmg.com/loss-functions-in-machine-learning'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://www.techopedia.com/definition/1221/normalization'",
        "'https://en.wikipedia.org/wiki/Error_function'",
        "'https://en.wikipedia.org/wiki/Error_function'",
        "'https://360digitmg.com/loss-functions-in-machine-learning', 'https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484', 'https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a'",
        "'https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/', 'https://360digitmg.com/loss-functions-in-machine-learning'",
        "'https://www.simplypsychology.org/p-value.html', 'https://en.wikipedia.org/wiki/Z-test', 'https://en.wikipedia.org/wiki/P-value'",
        "'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.', 'https://statisticsglobe.com/mode-imputation/', 'https://en.wikipedia.org/wiki/Error_function'",
        "'https://support.industry.siemens.com/cs/document/109759445/standardization-as-the-basis-of-digitalization-?dti=0&lc=en-WW', 'https://www.ibm.com/support/pages/pairwise-vs-listwise-deletion-what-are-they-and-when-should-i-use-them', 'https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114', 'https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114'",
        "'https://mode.com/', 'https://www.merriam-webster.com/dictionary/range', 'http://scikit-learn.org/stable/modules/svm.html'",
        "'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://en.wikipedia.org/wiki/Skewness', 'https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a'",
        "'https://en.wikipedia.org/wiki/Normal_distribution', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-multiple-linear-regression/'",
        "'https://en.wikipedia.org/wiki/Variance', 'https://www.investopedia.com/terms/c/coefficient-of-determination.asp', 'https://www.ibm.com/support/pages/pairwise-vs-listwise-deletion-what-are-they-and-when-should-i-use-them'",
        "'https://en.wikipedia.org/wiki/Mathematical_optimization'",
        "",
        "",
        "'https://www.merriam-webster.com/dictionary/range', 'https://www.geeksforgeeks.org/association-rule/', 'https://danielmiessler.com/blog/difference-median-mean/'",
        "",
        "'https://www.thelearningpoint.net/computer-science/python-for-data-8-ada-grad-vs-bold-driver-for-linear-classification'",
        "'https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html', 'https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/', 'https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa'",
        "'https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8'",
        "'https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646', 'https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/', 'https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863'",
        "'https://www.techopedia.com/definition/1221/normalization', 'https://support.industry.siemens.com/cs/document/109759445/standardization-as-the-basis-of-digitalization-?dti=0&lc=en-WW', 'https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa'",
        "'https://www.techopedia.com/definition/1221/normalization', 'https://support.industry.siemens.com/cs/document/109759445/standardization-as-the-basis-of-digitalization-?dti=0&lc=en-WW', 'https://www.geeksforgeeks.org/regularization-in-machine-learning/'",
        "'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/', 'https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/#:~:text=ExponentialDecay%20class&text=A%20LearningRateSchedule%20that%20uses%20an,a%20provided%20initial%20learning%20rate.', 'https://en.wikipedia.org/wiki/Polynomial_regression'",
        "'https://www.mathsisfun.com/data/probability.html', 'https://en.wikipedia.org/wiki/Probability', 'https://www.c-sharpcorner.com/article/importance-of-probability-in-machine-learning-and-data-science/'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-multiple-linear-regression/', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp'",
        "'https://methods.sagepub.com/reference/encyc-of-research-design/n329.xml', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.techopedia.com/definition/33181/training-data'",
        "'https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/', 'https://stattrek.com/probability-distributions/negative-binomial.aspx', 'https://en.wikipedia.org/wiki/Binomial_distribution'",
        "'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/', 'https://stattrek.com/probability-distributions/negative-binomial.aspx'",
        "'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://www.questionpro.com/blog/simple-random-sampling/'",
        "'https://www.merriam-webster.com/dictionary/range', 'https://danielmiessler.com/blog/difference-median-mean/', 'https://en.wikipedia.org/wiki/Median'",
        "'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://towardsdatascience.com/note-statistical-inference-the-big-picture-b1c1c4099cc7', 'https://seeing-theory.brown.edu/frequentist-inference/index.html'",
        "'https://www.opencodez.com/software-testing/test-data.htm', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://en.wikipedia.org/wiki/Named-entity_recognition'",
        "'https://www.statisticshowto.com/probability-and-statistics/types-of-variables/explanatory-variable/', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848', 'https://en.wikipedia.org/wiki/Big_data'",
        "'https://www.merriam-webster.com/dictionary/range', 'https://www.xenonstack.com/blog/difference-between-nlp-nlu-nlg', 'https://mode.com/'",
        "'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://www.statisticshowto.com/probability-and-statistics/types-of-variables/explanatory-variable/', 'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/'",
        "'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/', 'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/', 'https://www.researchgate.net/figure/Illustration-of-Sampling-Bias_fig2_342587109'",
        "'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/', 'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/', 'https://www.jmp.com/en_in/statistics-knowledge-portal/t-test/two-sample-t-test.html'",
        "'https://www.questionpro.com/blog/simple-random-sampling/', 'https://www.researchgate.net/figure/Illustration-of-Sampling-Bias_fig2_342587109', 'https://www.researchgate.net/figure/Illustration-of-Sampling-Bias_fig2_342587109'",
        "'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848', 'https://danielmiessler.com/blog/difference-median-mean/', 'https://pubmed.ncbi.nlm.nih.gov/17156271/'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://mode.com/'",
        "'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier'",
        "'https://stefvanbuuren.name/fimd/sec-MCAR.html', 'https://stefvanbuuren.name/fimd/sec-MCAR.html', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp'",
        "'https://corporatefinanceinstitute.com/resources/knowledge/other/central-tendency/', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848', 'https://en.wikipedia.org/wiki/Median'",
        "'https://mode.com/', 'https://www.merriam-webster.com/dictionary/range', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848'",
        "'https://danielmiessler.com/blog/difference-median-mean/', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848', 'https://www.sciencedirect.com/topics/engineering/class-variance'",
        "'https://www.investopedia.com/terms/c/coefficient-of-determination.asp', 'https://www.investopedia.com/ask/answers/012615/whats-difference-between-rsquared-and-adjusted-rsquared.asp', 'https://en.wikipedia.org/wiki/Standard_deviation'",
        "'https://en.wikipedia.org/wiki/Standard_deviation', 'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://en.wikipedia.org/wiki/Variance'",
        "'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp', 'https://towardsdatascience.com/note-statistical-inference-the-big-picture-b1c1c4099cc7'",
        "'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://en.wikipedia.org/wiki/Standard_deviation', 'https://en.wikipedia.org/wiki/Variance'",
        "'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://www.statisticshowto.com/mean-error/', 'https://www.investopedia.com/terms/c/coefficient-of-determination.asp'",
        "'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://www.investopedia.com/terms/c/conditional_probability.asp'",
        "",
        "'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://en.wikipedia.org/wiki/Linear_interpolation', 'https://www.questionpro.com/blog/interval-data/'",
        "",
        "'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9', 'https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/'",
        "",
        "'https://www.merriam-webster.com/dictionary/range', 'https://www.investopedia.com/terms/g/goodness-of-fit.asp'",
        "'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Maximum_likelihood_estimation'",
        "'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://www.ibm.com/docs/en/SSEPGG_9.7.0/com.ibm.im.model.doc/c_lift_in_an_association_rule.html', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1'",
        "'https://www.simplypsychology.org/confidence-interval.html', 'https://en.wikipedia.org/wiki/Interquartile_range', 'https://en.wikipedia.org/wiki/Polynomial_regression'",
        "'https://en.wikipedia.org/wiki/Probability', 'https://www.mathsisfun.com/data/probability.html', 'https://www.c-sharpcorner.com/article/importance-of-probability-in-machine-learning-and-data-science/'",
        "",
        "'https://odsc.medium.com/when-to-use-survival-analysis-applications-in-industry-data-science-d8bfb7566f43'",
        "'https://en.wikipedia.org/wiki/Hypothesis', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors'",
        "'https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce'",
        "",
        "'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9', 'https://www.simplypsychology.org/confidence-interval.html', 'https://seeing-theory.brown.edu/frequentist-inference/index.html'",
        "'https://en.wikipedia.org/wiki/Standard_deviation', 'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/'",
        "'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9', 'https://www.investopedia.com/terms/t/t-test.asp', 'https://towardsdatascience.com/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce'",
        "'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9', 'https://en.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.differencebetween.com/difference-between-time-series-and-cross-sectional-data/'",
        "'https://meganesilvey.wixsite.com/mysite/post/statistics-lesson-3-discrete-vs.-continuous-random-variables', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp'",
        "",
        "",
        "",
        "'https://www.simplypsychology.org/confidence-interval.html', 'https://www.c-sharpcorner.com/article/importance-of-probability-in-machine-learning-and-data-science/', 'https://en.wikipedia.org/wiki/Precision_and_recall'",
        "",
        "",
        "'https://en.wikipedia.org/wiki/Precision_and_recall', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229'",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Probability', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors'",
        "'https://en.wikipedia.org/wiki/Error_function', 'https://www.statisticshowto.com/mean-error/'",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://www.geeksforgeeks.org/association-rule/', 'https://www.simplypsychology.org/confidence-interval.html', 'https://seeing-theory.brown.edu/frequentist-inference/index.html'",
        "'https://www.sciencedirect.com/topics/mathematics/imputation-method', 'https://www.statisticshowto.com/mean-error/'",
        "'https://en.wikipedia.org/wiki/Standard_deviation', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://www.researchgate.net/figure/Descriptive-statistics-This-table-reports-descriptive-statistics-on-firm_tbl3_227450848'",
        "'https://www.investopedia.com/ask/answers/012615/whats-difference-between-rsquared-and-adjusted-rsquared.asp', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://stefvanbuuren.name/fimd/sec-MCAR.html'",
        "'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://www.statisticshowto.com/mean-error/', 'https://www.simplypsychology.org/confidence-interval.html'",
        "'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://www.investopedia.com/terms/c/coefficient-of-determination.asp', 'https://www.youtube.com/watch?v=A82brFpdr9g'",
        "'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/', 'https://www.investopedia.com/terms/g/goodness-of-fit.asp'",
        "'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229'",
        "'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://medium.com/analytics-vidhya/feature-engineering-part-1-mean-median-imputation-761043b95379', 'https://en.wikipedia.org/wiki/Median'",
        "'https://www.youtube.com/watch?v=Ilg3gGewQ5U'",
        "'https://www.geeksforgeeks.org/spatial-resolution-down-sampling-and-up-sampling-in-image-processing/', 'https://www.sciencedirect.com/topics/earth-and-planetary-sciences/image-enhancement#:~:text=5.2%20Image%20Enhancement,%2C%20density%20slicing%2C%20and%20FCC.', 'https://www.google.com/search?q=effect+of+anti+aliasing+filter+in+image+processing&sxsrf=AOaemvLpnjaZ107eyHUxh-itsI2P4FUPXQ%3A1641912703123&ei=f5ndYdLsBoyQseMP-pmbyAw&oq=effect+of+anti-aliasing+filter+in+im&gs_lcp=Cgdnd3Mtd2l6EAEYATIICCEQFhAdEB4yCAghEBYQHRAeMggIIRAWEB0QHjoHCAAQRxCwAzoGCAAQFhAeSgQIQRgASgQIRhgAUKETWKQvYJRZaAFwAHgAgAHfAYgB5weSAQUwLjUuMZgBAKABAcgBCMABAQ&sclient=gws-wiz'",
        "'https://statisticsglobe.com/mode-imputation/', 'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.', 'https://360digitmg.com/moments-of-business-decision-interview-questions'",
        "'https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/', 'https://www.geeksforgeeks.org/spatial-resolution-down-sampling-and-up-sampling-in-image-processing/', 'https://en.wikipedia.org/wiki/Error_function'",
        "'https://statisticsglobe.com/mode-imputation/', 'https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863', 'https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/'",
        "'https://www.sciencedirect.com/topics/medicine-and-dentistry/noise-reduction', 'https://www.geeksforgeeks.org/spatial-resolution-down-sampling-and-up-sampling-in-image-processing/', 'https://pjreddie.com/darknet/imagenet/'",
        "",
        "",
        "",
        "'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://en.wikipedia.org/wiki/Error_function', 'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/'",
        "'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.', 'https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d', 'https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/'",
        "'https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568', 'https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm', 'https://cambridge-intelligence.com/keylines-faqs-social-network-analysis/'",
        "",
        "'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.', 'https://www.simplypsychology.org/confidence-interval.html', 'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/'",
        "'https://statisticsglobe.com/mode-imputation/', 'https://www.statisticshowto.com/mean-error/', 'https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.'",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://statisticsglobe.com/mode-imputation/'",
        "'https://www.techopedia.com/definition/1221/normalization', 'https://statisticsglobe.com/mode-imputation/', 'https://www.talend.com/resources/data-transformation-defined/'",
        "'https://www.youtube.com/watch?v=A82brFpdr9g'",
        "'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/', 'https://en.wikipedia.org/wiki/Precision_and_recall'",
        "'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://en.wikipedia.org/wiki/Standard_deviation', 'https://www.statisticshowto.com/mean-error/'",
        "'https://en.wikipedia.org/wiki/F-score', 'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://www.investopedia.com/terms/c/coefficient-of-determination.asp'",
        "'https://www.statisticshowto.com/mean-error/', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Precision_and_recall'",
        "'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://machinelearningmastery.com/what-is-imbalanced-classification/', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://www.questionpro.com/blog/simple-random-sampling/', 'https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/degrees-of-freedom/', 'https://www.merriam-webster.com/dictionary/range'",
        "'https://www.simplypsychology.org/confidence-interval.html', 'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://stefvanbuuren.name/fimd/sec-MCAR.html'",
        "'https://www.researchgate.net/publication/317061892_Ensemble_Sampling', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/'",
        "'https://en.wikipedia.org/wiki/Standard_deviation', 'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9'",
        "'https://iq.opengenus.org/types-of-autoencoder/', 'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://en.wikipedia.org/wiki/Standard_deviation'",
        "'https://machinelearningmastery.com/cross-entropy-for-machine-learning/', 'https://iq.opengenus.org/types-of-autoencoder/', 'https://www.statisticshowto.com/probability-and-statistics/types-of-variables/explanatory-variable/'",
        "'https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/degrees-of-freedom/', 'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://www.jmp.com/en_in/statistics-knowledge-portal/t-test/two-sample-t-test.html'",
        "'https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://www.statisticshowto.com/winsorize/'",
        "'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/', 'https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa'",
        "'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9'",
        "'https://methods.sagepub.com/reference/encyclopedia-of-measurement-and-statistics/n473.xml', 'https://www.ibm.com/docs/en/SSEPGG_9.7.0/com.ibm.im.model.doc/c_lift_in_an_association_rule.html', 'https://en.wikipedia.org/wiki/Listwise_deletion'",
        "'https://en.wikipedia.org/wiki/Error_function'",
        "'https://360digitmg.com/moments-of-business-decision-interview-questions'",
        "'https://intellica-ai.medium.com/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c'",
        "'https://en.wikipedia.org/wiki/Normal_distribution', 'https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9', 'https://www.ibm.com/docs/en/spss-statistics/beta?topic=regression-multinomial-logistic'",
        "'https://www.investopedia.com/terms/p/probabilitydistribution.asp', 'https://www.investopedia.com/terms/p/probabilitydistribution.asp'",
        "'https://www.youtube.com/watch?v=A82brFpdr9g', 'https://en.wikipedia.org/wiki/F-score', 'https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/'",
        "'https://en.wikipedia.org/wiki/Error_function', 'https://en.wikipedia.org/wiki/Variance', 'https://en.wikipedia.org/wiki/F-score'",
        "'https://stefvanbuuren.name/fimd/sec-MCAR.html', 'https://www.google.com/search?rlz=1C1FHFK_enIN985IN985&sxsrf=AOaemvLElaoxMhrerEYD9_QQ_JyU4FXj-A:1641900589505&q=What+is+meant+by+prediction+accuracy%3F&sa=X&ved=2ahUKEwiI9ZKrzKn1AhXGyzgGHaWVCpAQzmd6BAhBEAU&biw=1366&bih=625&dpr=1', 'https://en.wikipedia.org/wiki/Precision_and_recall'",
        "'https://www.simplypsychology.org/confidence-interval.html', 'https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5', 'https://www.ibm.com/docs/en/spss-statistics/beta?topic=regression-multinomial-logistic'",
        "'https://www.simplypsychology.org/confidence-interval.html', 'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9', 'https://mode.com/'",
        "'https://www.ibm.com/docs/en/SSEPGG_9.7.0/com.ibm.im.model.doc/c_lift_in_an_association_rule.html', 'https://www.simplypsychology.org/confidence-interval.html', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229'",
        "'https://simplicable.com/new/business-constraints'",
        "",
        "'https://www.yourarticlelibrary.com/business/business-objectives-5-most-important-objectives-of-business/23362', 'https://www.oracle.com/in/data-science/'",
        "'https://www.investopedia.com/terms/b/business-intelligence-bi.asp', 'https://www.oracle.com/in/data-science/', 'https://www.researchgate.net/figure/Business-intelligence-business-analytics-breakdown-Klimberg-Miori-2010_fig1_276491531'",
        "'https://examples.yourdictionary.com/examples-of-hypothesis-testing.html', 'https://en.wikipedia.org/wiki/Hypothesis', 'https://seeing-theory.brown.edu/frequentist-inference/index.html'",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.avercast.in/blog/what-is-naive-forecasting-and-how-can-be-used-to-calculate-future-demand'",
        "'https://www.statisticshowto.com/mean-error/', 'https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229', 'https://www.simplypsychology.org/confidence-interval.html'",
        "'https://www.avercast.in/blog/what-is-naive-forecasting-and-how-can-be-used-to-calculate-future-demand', 'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://methods.sagepub.com/reference/encyc-of-research-design/n329.xml'",
        "'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/', 'https://360digitmg.com/loss-functions-in-machine-learning', 'https://www.oracle.com/in/data-science/'",
        "",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.ibm.com/docs/SSEPGG_10.1.0/com.ibm.im.model.doc/c_confidence_in_an_association_rule.html'",
        "",
        "'https://en.wikipedia.org/wiki/Model_selection', 'https://www.ibm.com/docs/en/spss-statistics/beta?topic=regression-multinomial-logistic'",
        "'https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/#:~:text=ExponentialDecay%20class&text=A%20LearningRateSchedule%20that%20uses%20an,a%20provided%20initial%20learning%20rate.'",
        "'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://en.wikipedia.org/wiki/Hypothesis', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors'",
        "'https://en.wikipedia.org/wiki/Hypothesis', 'https://www.investopedia.com/terms/c/conditional_probability.asp', 'https://en.wikipedia.org/wiki/Bayes%27_theorem'",
        "'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/', 'https://en.wikipedia.org/wiki/Model_selection'",
        "'https://en.wikipedia.org/wiki/Hypothesis', 'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors'",
        "'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival_print.html'",
        "'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://seeing-theory.brown.edu/frequentist-inference/index.html', 'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9'",
        "'https://odsc.medium.com/when-to-use-survival-analysis-applications-in-industry-data-science-d8bfb7566f43', 'https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/', 'https://www.geeksforgeeks.org/association-rule/'",
        "",
        "'https://www.khanacademy.org/math/old-ap-calculus-ab/ab-derivative-rules/ab-diff-negative-fraction-powers/a/power-rule-review', 'https://www.oracle.com/in/data-science/'",
        "'https://www.simplypsychology.org/type_I_and_type_II_errors.html', 'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://www.simplypsychology.org/type_I_and_type_II_errors.html'",
        "'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/'",
        "'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/'",
        "'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors', 'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/'",
        "'https://www.investopedia.com/terms/t/type-ii-error.asp', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/', 'https://en.wikipedia.org/wiki/Type_I_and_type_II_errors'",
        "'https://www.investopedia.com/terms/g/goodness-of-fit.asp', 'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://www.opencodez.com/software-testing/test-data.htm'",
        "'https://pubmed.ncbi.nlm.nih.gov/17156271/', 'https://en.wikipedia.org/wiki/Bayes%27_theorem', 'https://sixsigmastudyguide.com/one-and-two-sample-proportion-hypothesis-tests/'",
        "'https://www.investopedia.com/terms/q/qualitativeanalysis.asp', 'https://examples.yourdictionary.com/examples-of-hypothesis-testing.html', 'https://pubmed.ncbi.nlm.nih.gov/17156271/'",
        "'https://en.wikipedia.org/wiki/Regression_analysis', 'https://stattrek.com/regression/linear-transformation.aspx', 'https://www.statisticshowto.com/probability-and-statistics/regression-analysis/find-a-linear-regression-equation/'",
        "'https://www.geeksforgeeks.org/stacking-in-machine-learning/', 'https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/', 'https://towardsdatascience.com/how-to-improve-the-accuracy-of-a-regression-model-3517accf8604'",
        "",
        "",
        "",
        "'https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf'",
        "'https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm', 'https://towardsdatascience.com/market-mix-modeling-mmm-101-3d094df976f9'",
        "'https://www.researchgate.net/figure/Business-intelligence-business-analytics-breakdown-Klimberg-Miori-2010_fig1_276491531', 'https://www.differencebetween.com/difference-between-time-series-and-cross-sectional-data/', 'https://www.nuance.com/omni-channel-customer-engagement/analytics-solutions.html'",
        "'https://www.displayr.com/deal-missing-values-cluster-analysis/', 'https://en.wikipedia.org/wiki/Listwise_deletion', 'https://www.statistics.com/glossary/hold-out-sample/'",
        "'https://www.analyticsvidhya.com/blog/2021/05/topic-modelling-in-natural-language-processing/', 'https://www.datascience-pm.com/crisp-dm-2/', 'https://ml-ops.org/content/crisp-ml'",
        "'https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/'",
        "",
        "'https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/', 'https://en.wikipedia.org/wiki/Polynomial_regression', 'https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html'",
        "'https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/#:~:text=Deep%20Learning%20has%20changed%20the%20game%20in%20speech,Baidu%2C%20and%20Listen%20Attend%20Spell%20%28LAS%29%20by%20Google.', 'https://www.researchgate.net/publication/234151337_On_Preprocessing_of_Speech_Signals'"
    ]
}